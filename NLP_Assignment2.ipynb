{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Assignment2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheoLiapik/Sidiras_Liapikos2/blob/master/NLP_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "V8h7BXNa_AHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://www.auth.gr/sites/default/files/banner-horizontal-282x100.png)\n",
        "# Text Mining and Natural Language Processing - Assignment 2\n",
        "\n",
        "\n",
        "** Text Classification using Word Embeddings**\n",
        "<br>\n",
        "**Potentially useful library documentation, references, and resources**:\n",
        "\n",
        "* IPython notebooks: <https://ipython.org/ipython-doc/3/notebook/notebook.html#introduction>\n",
        "* Numpy numerical array library: <https://docs.scipy.org/doc/>\n",
        "* Gensim's word2vec: <https://radimrehurek.com/gensim/models/word2vec.html>\n",
        "* Keras Deep-Learning library: <https://keras.io/layers/embeddings/>\n",
        "* Bokeh interactive plots: <http://bokeh.pydata.org/en/latest/> (we provide plotting code here, but click the thumbnails for more examples to copy-paste)\n",
        "* scikit-learn ML library (aka `sklearn`): <http://scikit-learn.org/stable/documentation.html>\n",
        "* nltk NLP toolkit: <http://www.nltk.org/>\n",
        "* tutorial for processing xml in python using `lxml`: <http://lxml.de/tutorial.html> (we did this for you below, but in case you need it in the future)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DpiibVTO3lMG",
        "colab_type": "code",
        "outputId": "ebf17822-06fb-4bc2-97a4-48e314cef284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import bokeh\n",
        "import gensim\n",
        "import numpy as np\n",
        "import re\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GsG--w-sOPzJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 1"
      ]
    },
    {
      "metadata": {
        "id": "8ywrCrDahvTk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1. Train a Word2Vec model on the WikiText dataset"
      ]
    },
    {
      "metadata": {
        "id": "fJztXrJRMy00",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 Download Dataset\n",
        "\n",
        "One could skip the next time consuming stages and upload the already pre-processed clean data directly from my personal Google Drive at section 1.1.3.3"
      ]
    },
    {
      "metadata": {
        "id": "FmnIJSbRdgWx",
        "colab_type": "code",
        "outputId": "1f1ac7ff-39cb-4cc2-d1a1-53d61e6f6046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Import necessary Libraries\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Download the dataset ~190MB\n",
        "urllib.request.urlretrieve(\"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\", filename=\"wikitext-103-v1.zip\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('wikitext-103-v1.zip', <http.client.HTTPMessage at 0x7efe78788470>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "u6anXhJHeMkZ",
        "colab_type": "code",
        "outputId": "aa6bd7ce-9e14-409e-aec5-00953e43e878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Extract only the data of interest\n",
        "# From the .zip file open and read only the training tokens\n",
        "with zipfile.ZipFile('wikitext-103-v1.zip', 'r') as z:\n",
        "  doc = z.open('wikitext-103/wiki.train.tokens', 'r').read()\n",
        "\n",
        "# The first 500 bytes of data\n",
        "print(doc[:500])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b' \\n = Valkyria Chronicles III = \\n \\n Senj\\xc5\\x8d no Valkyria 3 : <unk> Chronicles ( Japanese : \\xe6\\x88\\xa6\\xe5\\xa0\\xb4\\xe3\\x81\\xae\\xe3\\x83\\xb4\\xe3\\x82\\xa1\\xe3\\x83\\xab\\xe3\\x82\\xad\\xe3\\x83\\xa5\\xe3\\x83\\xaa\\xe3\\x82\\xa23 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TiCt3-UgM5IK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 Data Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "Ylhj68BonFRe",
        "colab_type": "code",
        "outputId": "4c808578-4abb-4655-e905-3e55c7046ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Convert bytes to string and then split to paragraphs\n",
        "doc_str = doc.decode(\"utf-8\")\n",
        "doc_para  = doc_str.split('\\n')\n",
        "\n",
        "# the first 5 paragraphs\n",
        "print(doc_para[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' ', ' = Valkyria Chronicles III = ', ' ', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . ', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C11cpwv1Q1TE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.2.1 Basic pre-processing procedure\n",
        "For each paragraph of the data:\n",
        "- Remove multiple space characters\n",
        "- Remove empty tokens\n",
        "- Lower the characters\n",
        "- Remove non-Alpharithmetic characters\n",
        "- Remove arithmetic words\n",
        "- Remove created multiple space characters\n",
        "- Tokenize (split on space characters)\n",
        "- Remove stop-words\n",
        "- Remove words with less than 3 characters\n",
        "- Remove empty paragraphs\n",
        "- Save the remaining tokens to a list\n"
      ]
    },
    {
      "metadata": {
        "id": "JyTt3FjVQzzN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc_para_noEmpties = []\n",
        "for para in doc_para:\n",
        "    para = re.sub(r'\\s+', ' ',para)\n",
        "    if para != ' ':\n",
        "        para = para.lower()\n",
        "        para = re.sub(r'[^a-z0-9]+', ' ',para)\n",
        "        para = re.sub(r' [0-9]+ ', ' ',para)\n",
        "        para = re.sub(r'\\s+', ' ',para)\n",
        "        para = para.split(' ')\n",
        "        para = [word for word in para if word not in stopwords.words('english')]\n",
        "        para = [word for word in para if len(word)>2]\n",
        "        if len(para) == 0:\n",
        "          continue\n",
        "        doc_para_noEmpties.append(para)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_SdSqbbTrvUw",
        "colab_type": "code",
        "outputId": "805f3ab3-164f-4f88-994b-0b71f430e79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# The first 6 paragraphs of clean data\n",
        "print(doc_para_noEmpties[:6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['valkyria', 'chronicles', 'iii'], ['senj', 'valkyria', 'unk', 'chronicles', 'japanese', 'lit', 'valkyria', 'battlefield', 'commonly', 'referred', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', 'tactical', 'role', 'playing', 'video', 'game', 'developed', 'sega', 'media', 'vision', 'playstation', 'portable', 'released', 'january', 'japan', 'third', 'game', 'valkyria', 'series', 'employing', 'fusion', 'tactical', 'real', 'time', 'gameplay', 'predecessors', 'story', 'runs', 'parallel', 'first', 'game', 'follows', 'nameless', 'penal', 'military', 'unit', 'serving', 'nation', 'gallia', 'second', 'europan', 'war', 'perform', 'secret', 'black', 'operations', 'pitted', 'imperial', 'unit', 'unk', 'raven'], ['game', 'began', 'development', 'carrying', 'large', 'portion', 'work', 'done', 'valkyria', 'chronicles', 'retained', 'standard', 'features', 'series', 'also', 'underwent', 'multiple', 'adjustments', 'making', 'game', 'forgiving', 'series', 'newcomers', 'character', 'designer', 'unk', 'honjou', 'composer', 'hitoshi', 'sakimoto', 'returned', 'previous', 'entries', 'along', 'valkyria', 'chronicles', 'director', 'takeshi', 'ozawa', 'large', 'team', 'writers', 'handled', 'script', 'game', 'opening', 'theme', 'sung', 'may'], ['met', 'positive', 'sales', 'japan', 'praised', 'japanese', 'western', 'critics', 'release', 'received', 'downloadable', 'content', 'along', 'expanded', 'edition', 'november', 'year', 'also', 'adapted', 'manga', 'original', 'video', 'animation', 'series', 'due', 'low', 'sales', 'valkyria', 'chronicles', 'valkyria', 'chronicles', 'iii', 'localized', 'fan', 'translation', 'compatible', 'game', 'expanded', 'edition', 'released', 'media', 'vision', 'would', 'return', 'franchise', 'development', 'valkyria', 'azure', 'revolution', 'playstation'], ['gameplay'], ['previous', 'unk', 'chronicles', 'games', 'valkyria', 'chronicles', 'iii', 'tactical', 'role', 'playing', 'game', 'players', 'take', 'control', 'military', 'unit', 'take', 'part', 'missions', 'enemy', 'forces', 'stories', 'told', 'comic', 'book', 'like', 'panels', 'animated', 'character', 'portraits', 'characters', 'speaking', 'partially', 'voiced', 'speech', 'bubbles', 'partially', 'unvoiced', 'text', 'player', 'progresses', 'series', 'linear', 'missions', 'gradually', 'unlocked', 'maps', 'freely', 'scanned', 'replayed', 'unlocked', 'route', 'story', 'location', 'map', 'varies', 'depending', 'individual', 'player', 'approach', 'one', 'option', 'selected', 'sealed', 'player', 'outside', 'missions', 'player', 'characters', 'rest', 'camp', 'units', 'customized', 'character', 'growth', 'occurs', 'alongside', 'main', 'story', 'missions', 'character', 'specific', 'sub', 'missions', 'relating', 'different', 'squad', 'members', 'game', 'completion', 'additional', 'episodes', 'unlocked', 'higher', 'difficulty', 'found', 'rest', 'game', 'also', 'love', 'simulation', 'elements', 'related', 'game', 'two', 'main', 'heroines', 'although', 'take', 'minor', 'role']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jg0jyPXgeFdl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.3 Store and Load Data Locally\n",
        "\n",
        "The duration of data pre-processing procedure is extremly long (> 4 h).  After completion I saved (takes a couple of minutes) the clean data to a .csv file, to use it locally from Drive.\n",
        "\n",
        "You can download the .csv file from this link:\n",
        "[preproc_clean_data.csv](https://drive.google.com/open?id=1vgObuKTZ0iL69AUWt_uZ6Orx49Lu1KfS)"
      ]
    },
    {
      "metadata": {
        "id": "DijjxUPfUWKZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.3.1 Connect to personal Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "SK0tl14Rirge",
        "colab_type": "code",
        "outputId": "9acddcc0-3755-4729-9912-9087ebeae212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "# Connect to personal Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OkxvT5okrlLh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.3.2 Store Clean Data to Drive"
      ]
    },
    {
      "metadata": {
        "id": "2KC_4pSmWisN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save data to local .csv file\n",
        "import csv\n",
        "\n",
        "with open(\"/content/drive/My Drive/NLP Assignment 2/preproc_clean_data.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(doc_para_noEmpties)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "02OiE2ZNb_Ck",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.3.3 Load Clean Data from Drive"
      ]
    },
    {
      "metadata": {
        "id": "6bCX1xgwn6gX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Recover data from local .csv file\n",
        "import csv\n",
        "\n",
        "doc_para_noEmpties = []\n",
        "with open(\"/content/drive/My Drive/NLP Assignment 2/preproc_clean_data.csv\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "        doc_para_noEmpties.append(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nkrt4uZtBLeL",
        "colab_type": "code",
        "outputId": "61726eaa-31cb-428b-c0c5-f83fb023a9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# The first 6 paragraphs from restored data\n",
        "print(doc_para_noEmpties[:6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['valkyria', 'chronicles', 'iii'], ['senj', 'valkyria', 'unk', 'chronicles', 'japanese', 'lit', 'valkyria', 'battlefield', 'commonly', 'referred', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', 'tactical', 'role', 'playing', 'video', 'game', 'developed', 'sega', 'media', 'vision', 'playstation', 'portable', 'released', 'january', 'japan', 'third', 'game', 'valkyria', 'series', 'employing', 'fusion', 'tactical', 'real', 'time', 'gameplay', 'predecessors', 'story', 'runs', 'parallel', 'first', 'game', 'follows', 'nameless', 'penal', 'military', 'unit', 'serving', 'nation', 'gallia', 'second', 'europan', 'war', 'perform', 'secret', 'black', 'operations', 'pitted', 'imperial', 'unit', 'unk', 'raven'], ['game', 'began', 'development', 'carrying', 'large', 'portion', 'work', 'done', 'valkyria', 'chronicles', 'retained', 'standard', 'features', 'series', 'also', 'underwent', 'multiple', 'adjustments', 'making', 'game', 'forgiving', 'series', 'newcomers', 'character', 'designer', 'unk', 'honjou', 'composer', 'hitoshi', 'sakimoto', 'returned', 'previous', 'entries', 'along', 'valkyria', 'chronicles', 'director', 'takeshi', 'ozawa', 'large', 'team', 'writers', 'handled', 'script', 'game', 'opening', 'theme', 'sung', 'may'], ['met', 'positive', 'sales', 'japan', 'praised', 'japanese', 'western', 'critics', 'release', 'received', 'downloadable', 'content', 'along', 'expanded', 'edition', 'november', 'year', 'also', 'adapted', 'manga', 'original', 'video', 'animation', 'series', 'due', 'low', 'sales', 'valkyria', 'chronicles', 'valkyria', 'chronicles', 'iii', 'localized', 'fan', 'translation', 'compatible', 'game', 'expanded', 'edition', 'released', 'media', 'vision', 'would', 'return', 'franchise', 'development', 'valkyria', 'azure', 'revolution', 'playstation'], ['gameplay'], ['previous', 'unk', 'chronicles', 'games', 'valkyria', 'chronicles', 'iii', 'tactical', 'role', 'playing', 'game', 'players', 'take', 'control', 'military', 'unit', 'take', 'part', 'missions', 'enemy', 'forces', 'stories', 'told', 'comic', 'book', 'like', 'panels', 'animated', 'character', 'portraits', 'characters', 'speaking', 'partially', 'voiced', 'speech', 'bubbles', 'partially', 'unvoiced', 'text', 'player', 'progresses', 'series', 'linear', 'missions', 'gradually', 'unlocked', 'maps', 'freely', 'scanned', 'replayed', 'unlocked', 'route', 'story', 'location', 'map', 'varies', 'depending', 'individual', 'player', 'approach', 'one', 'option', 'selected', 'sealed', 'player', 'outside', 'missions', 'player', 'characters', 'rest', 'camp', 'units', 'customized', 'character', 'growth', 'occurs', 'alongside', 'main', 'story', 'missions', 'character', 'specific', 'sub', 'missions', 'relating', 'different', 'squad', 'members', 'game', 'completion', 'additional', 'episodes', 'unlocked', 'higher', 'difficulty', 'found', 'rest', 'game', 'also', 'love', 'simulation', 'elements', 'related', 'game', 'two', 'main', 'heroines', 'although', 'take', 'minor', 'role']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W1cBbTCfjwUH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.4 Training the Word2Vec model on Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Wqbhrp28kt0D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.4.1 Setting model's parameters:\n",
        "- **window** (int, optional) – Maximum distance between the current and predicted word within a sentence\n",
        "- **size** (int, optional) – Dimensionality of the word vectors\n",
        "- **sg** ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW\n",
        "- **min_count** (int, optional) – Ignores all words with total frequency lower than this\n",
        "- **workers** (int, optional) – Use these many worker threads to train the model"
      ]
    },
    {
      "metadata": {
        "id": "olFLWr9qh0fl",
        "colab_type": "code",
        "outputId": "460f87fe-2c16-45cc-994e-49958b13a15f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(window=4,size=100,sg=1,min_count=10,workers = -1)\n",
        "model.build_vocab(doc_para_noEmpties)  # Building the model vocabulary\n",
        "model.train(doc_para_noEmpties,total_examples=model.corpus_count,epochs=model.iter)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "lBszLshgh0Y6",
        "colab_type": "code",
        "outputId": "d707786b-0326-48b4-fbc4-9be6363e51ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# Model's Vocabulary\n",
        "vocab = model.wv.vocab\n",
        "print('Vocabulary size:')\n",
        "print(len(vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:\n",
            "111839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ciRyk-iqh0Oh",
        "colab_type": "code",
        "outputId": "ff1e2d11-3b4b-4169-d8d6-54f63d9b184c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# 10 most frequent words\n",
        "mfw = model.wv.index2word[:10]\n",
        "print(mfw)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['unk', 'first', 'one', 'also', 'two', 'new', 'time', 'would', 'game', 'later']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7SzqYCvnpb5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.5 Find the 5 most similar word pairs from the 10 most frequent words"
      ]
    },
    {
      "metadata": {
        "id": "bBToNIs3h0Hb",
        "colab_type": "code",
        "outputId": "199f562b-da6b-4bdb-c5f9-136cf2e363da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "cell_type": "code",
      "source": [
        "import operator\n",
        "\n",
        "# Use dictionary structure to store word pairs and their similarity\n",
        "similar_pairs = {}\n",
        "\n",
        "# Compare each of the 10 most frequent words against the others and find similarity\n",
        "for p1 in mfw:\n",
        "  # restrict_vocab parameter restricts comparison to the 10 most frequent words\n",
        "    pairs = model.wv.most_similar(p1, restrict_vocab=10)\n",
        "    for p2,sim in pairs:\n",
        "        similar_pairs[(p1, p2)] = sim\n",
        "\n",
        "# Sort dictionary's data by values\n",
        "sorted_similar_pairs = sorted(similar_pairs.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "# Since each pair appears twice, I keep every other element of the ordered list\n",
        "sorted_similar_pairs = sorted_similar_pairs[0:10:2]\n",
        "\n",
        "# Print pairs\n",
        "for pair in sorted_similar_pairs:\n",
        "  print(pair)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('unk', 'first'), 0.19137097895145416)\n",
            "(('one', 'time'), 0.18669351935386658)\n",
            "(('unk', 'later'), 0.1399707943201065)\n",
            "(('also', 'would'), 0.11235056817531586)\n",
            "(('unk', 'new'), 0.10211379081010818)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "n7JaKzUf0wzp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CuyBXtMrvjfC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2. Implement a function that retrieves two word vectors and computes their cosine distance"
      ]
    },
    {
      "metadata": {
        "id": "st2AsYNthz_b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A function taking as input a trained word2vec model and two words (strings) and\n",
        "# then manually computes and returns their cosine distance\n",
        "def cosVecDist(w2vModel, str1, str2):\n",
        "    # Use model to find vector representation of strings\n",
        "    vstr1 = w2vModel.wv[str1]\n",
        "    vstr2 = w2vModel.wv[str2]\n",
        "    # Cosine distance of two vectors equals to the dot product of vectors\n",
        "    # divided by the product of vectrors' legths\n",
        "    # The dot product of two 1-D vectors can be computed using numpy.dot() function\n",
        "    dotProd = np.dot(vstr1,vstr2)\n",
        "    # The length of a 1-D vector can be computed using numpy.linalg.norm() function\n",
        "    # The default value of ord parameter (ord=None), returns the 2-norm of vectors\n",
        "    length1 = np.linalg.norm(vstr1, ord=None)\n",
        "    length2 = np.linalg.norm(vstr2, ord=None)\n",
        "    # Cosine distance of the vectors\n",
        "    cosDist = dotProd/(length1*length2)\n",
        "    return(cosDist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_r14X-pAxXOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Comparing the two approaches\n",
        "\n",
        "Compute Cosine Distance of word pairs using 2 methods:\n",
        "\n",
        "- Custom cosVecDist() function defined above\n",
        "- The in-built wv.similarity() function of trained model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "55tWMmEyqSEO",
        "outputId": "ed48c280-52f5-42a3-e37e-04ea20d0ed7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# I use the trained model from previous step\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Create 5 random pairs of words from model's vocabulary\n",
        "vocab = model.wv.index2word\n",
        "list = []\n",
        "for i in range(5):\n",
        "  list.append(random.choices(vocab, k=2))\n",
        "\n",
        "# DataFrame to store the results\n",
        "headers = ['Word 1', 'Word 2', 'Custom Function', 'Model\\'s Function']\n",
        "cosDist = pd.DataFrame(columns = headers)\n",
        "\n",
        "for pair in list:\n",
        "  results = []\n",
        "  results.append(pair[0])\n",
        "  results.append(pair[1])\n",
        "  results.append(cosVecDist(model, pair[0], pair[1]))\n",
        "  results.append(model.wv.similarity(pair[0], pair[1]))\n",
        "  \n",
        "  new_row = pd.Series(results, index = headers)\n",
        "  cosDist = cosDist.append(new_row, ignore_index=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "H_70QU3ayh6t",
        "colab_type": "code",
        "outputId": "3e0f2118-478c-4930-df78-31deb5798a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "print('Computing Cosine Distance with 2 different methods:')\n",
        "cosDist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing Cosine Distance with 2 different methods:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word 1</th>\n",
              "      <th>Word 2</th>\n",
              "      <th>Custom Function</th>\n",
              "      <th>Model's Function</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>flameout</td>\n",
              "      <td>fertilise</td>\n",
              "      <td>-0.171696</td>\n",
              "      <td>-0.171696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>belmonte</td>\n",
              "      <td>mocking</td>\n",
              "      <td>0.049636</td>\n",
              "      <td>0.049636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>leg</td>\n",
              "      <td>platelets</td>\n",
              "      <td>0.099155</td>\n",
              "      <td>0.099155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>supple</td>\n",
              "      <td>romani</td>\n",
              "      <td>-0.040979</td>\n",
              "      <td>-0.040979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pensford</td>\n",
              "      <td>bandshell</td>\n",
              "      <td>-0.030766</td>\n",
              "      <td>-0.030766</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Word 1     Word 2  Custom Function  Model's Function\n",
              "0  flameout  fertilise        -0.171696         -0.171696\n",
              "1  belmonte    mocking         0.049636          0.049636\n",
              "2       leg  platelets         0.099155          0.099155\n",
              "3    supple     romani        -0.040979         -0.040979\n",
              "4  pensford  bandshell        -0.030766         -0.030766"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "eHOp5OQE1P7r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Conclusions\n",
        "\n",
        "Both methods delivers exactly the same result, so it's obvious that in-built wv.similarity() function uses cosine distance to express the similarity of two words"
      ]
    },
    {
      "metadata": {
        "id": "4htm8GBBgwQG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z_ANqLSxMarZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1.3.     Visit the NLPL word embedding repository and download the models with the following identifiers: \n",
        "\n",
        "- 40. It was trained on the English CoNLL17 corpus, using Continuous Skip-gram algorithm with vector size 100, and window size 10.\n",
        "- 75. It was trained on the English Oil and Gas corpus, using Continuous Bag-of-Words algorithm with vector size 400, and window size 5.\n",
        "- 82. It was trained on the English Common Crawl Corpus, using GloVe algorithm with vector size 300, and window size 10."
      ]
    },
    {
      "metadata": {
        "id": "IkgUSbdINvOs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3NRo4v5qf5z2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3.1 CoNLL17 corpus"
      ]
    },
    {
      "metadata": {
        "id": "ChjgZImvM-cY",
        "colab_type": "code",
        "outputId": "c7ae1151-e926-4147-d759-61ae08e96413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading CoNLL17 corpus ~1.5GB\n",
        "urllib.request.urlretrieve(\"http://vectors.nlpl.eu/repository/11/40.zip\", filename=\"40.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('40.zip', <http.client.HTTPMessage at 0x7eff2e30e198>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "KJYLIL4vgB34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3.2 Oil and Gas corpus"
      ]
    },
    {
      "metadata": {
        "id": "xJlfAgU1M-DX",
        "colab_type": "code",
        "outputId": "cdc32754-2673-49db-f5aa-a343933db93b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading Oil and Gas corpus ~0.4GB\n",
        "urllib.request.urlretrieve(\"http://vectors.nlpl.eu/repository/11/75.zip\", filename=\"75.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('75.zip', <http.client.HTTPMessage at 0x7efe7872f128>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "cW1qJ2IEgEFz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3.3 Common Crawl corpus"
      ]
    },
    {
      "metadata": {
        "id": "WpNsKTukxUYt",
        "colab_type": "code",
        "outputId": "1c46e9a2-8624-4314-8fea-1d0b6f27dda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading Oil and Gas corpus ~2.3GB\n",
        "urllib.request.urlretrieve(\"http://vectors.nlpl.eu/repository/11/82.zip\", filename=\"82.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('82.zip', <http.client.HTTPMessage at 0x7efe7872fdd8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "k7HTMT6tgtKe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xlton0kxNAC4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1.4. Create the lists of top 20 most frequent words in WikiText, CoNLL17, Oil and Gas, and Common Crawl corpora"
      ]
    },
    {
      "metadata": {
        "id": "kMRXghbfPDVs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z_Uw9qYMhz4U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dictionary structure to store 20 most frequent words of each corpus\n",
        "twentyFreqWords = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eq5EROnfbr4w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 Wiki Text"
      ]
    },
    {
      "metadata": {
        "id": "FZX6o7S_hMc1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Just use the wv.index2word function on the model trained at stage 1.4"
      ]
    },
    {
      "metadata": {
        "id": "wYvIWs6dcxP4",
        "colab_type": "code",
        "outputId": "8d7ca449-d7c8-4eeb-9e35-c0ddf1bbca4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# 20 most frequent words\n",
        "wiki_mfw = model.wv.index2word[:20]\n",
        "twentyFreqWords['WikiText'] = wiki_mfw\n",
        "print(twentyFreqWords['WikiText'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['unk', 'first', 'one', 'also', 'two', 'new', 'time', 'would', 'game', 'later', 'three', 'film', 'may', 'year', 'made', 'second', 'season', 'years', 'world', 'war']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u_7MXE1EiKUL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the downloaded pre-trained embeddings I follow a differnt approach. Since each model is saved as a text file in the word2vec format, its lines are as a rule sorted by frequency. Each word and its vector corresponds to a paragraph of the .txt file. Furthermore, the first paragraph of the .txt file holds informations about the length of the vocabulary and the dimensionality of the vectors. So I just have to read the first few ten thousands of bytes, accordind to their vector's dimensionality and make sure to include at least the first 22 paragraphs.\n",
        "\n",
        "For each model I repeat the following steps:\n",
        "- Extract and read part of the model.txt file stored in the downloaded .zip file\n",
        "- Convert the text bytes to string and split to paragraphs \n",
        "- Tokenize each paragraph to words\n",
        "- Ignore the first paragraph and keep the first word of the following 20 paragraphs to list\n"
      ]
    },
    {
      "metadata": {
        "id": "n5wDtjHeUMZI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 CoNLL17 corpus"
      ]
    },
    {
      "metadata": {
        "id": "Xt5M1d9Yhzp2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use the model.txt file inside the .zip file\n",
        "# Read only the first 20000 bytes\n",
        "with zipfile.ZipFile('40.zip', 'r') as z:\n",
        "  doc40 = z.open('model.txt', 'r').read(20000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4Xgtb-7TVoW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert bytes to string and then split to paragraphs\n",
        "doc_str = doc40.decode(\"utf-8\")\n",
        "doc_para  = doc_str.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uigW1pb8hzh9",
        "colab_type": "code",
        "outputId": "5ef86049-0554-423e-d24e-fceb93acfaa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Check if sample includes at least the first 22 paragraphs\n",
        "len(doc_para)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "EBLkJMD5FZK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "para_list = []\n",
        "for i in range(20):\n",
        "  # Ignore the first paragraph\n",
        "  para = doc_para[i+1].split(' ')[0]\n",
        "  para_list.append(para)\n",
        "\n",
        "# Save list to dictionary\n",
        "twentyFreqWords['40:CoNLL15'] = para_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KelxareuaPSo",
        "colab_type": "code",
        "outputId": "508f9736-66e3-4d08-fc92-267b99af6285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(twentyFreqWords['40:CoNLL15'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['</s>', ',', 'the', '.', 'of', 'and', 'to', 'a', 'in', '-', ')', '(', ':', 'for', 'is', '\"', 'on', 'i', 'that', 'with']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gU09fUxWUaVY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4.3 Oil and Gas corpus"
      ]
    },
    {
      "metadata": {
        "id": "21W_tU1HU6Vd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Same procedure as above"
      ]
    },
    {
      "metadata": {
        "id": "vs52GZpHEWTH",
        "colab_type": "code",
        "outputId": "525ff2ee-8634-4e74-e3a2-23d0c56b5c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('75.zip', 'r') as z:\n",
        "  doc75 = z.open('model.txt', 'r').read(80000)\n",
        "\n",
        "doc_str = doc75.decode(\"utf-8\")\n",
        "doc_para  = doc_str.split('\\n')\n",
        "\n",
        "len(doc_para)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "BR3ftSkeRLvo",
        "colab_type": "code",
        "outputId": "30f9ef86-c744-4062-b172-4d6d5f2dbe2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "para_list = []\n",
        "for i in range(20):\n",
        "  para = doc_para[i+1].split(' ')[0]\n",
        "  para_list.append(para)\n",
        "\n",
        "twentyFreqWords['75:OilAndGas'] = para_list\n",
        "\n",
        "print(twentyFreqWords['75:OilAndGas'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lrb', 'rrb', 'sediment', 'fault', 'datum', 'basin', 'sample', 'area', 'study', 'model', 'result', 'zone', 'water', 'rock', 'time', 'formation', 'high', 'surface', 'increase', 'change']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LE6H-iUFVnyE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4.4 Common Crawl corpus"
      ]
    },
    {
      "metadata": {
        "id": "CVqMzsupVyca",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Same procedure as above"
      ]
    },
    {
      "metadata": {
        "id": "9CDkr2eRhzY2",
        "colab_type": "code",
        "outputId": "de3adc57-670b-47a2-aee7-ffaf0a9426c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('82.zip', 'r') as z:\n",
        "  doc82 = z.open('model.txt', 'r').read(60000)\n",
        "  \n",
        "doc_str = doc82.decode(\"utf-8\")\n",
        "doc_para  = doc_str.split('\\n')\n",
        "\n",
        "len(doc_para)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "5GyqryFnWuIC",
        "colab_type": "code",
        "outputId": "d0a35d20-38b0-4ec7-a100-c4a9eb46743e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "para_list = []\n",
        "for i in range(20):\n",
        "  para = doc_para[i+1].split(' ')[0]\n",
        "  para_list.append(para)\n",
        "\n",
        "twentyFreqWords['82:CommonCrawl'] = para_list\n",
        "\n",
        "print(twentyFreqWords['82:CommonCrawl'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', ',', '.', 'and', 'to', 'of', 'a', 'in', 'is', 'that', 'i', 'for', 'it', 'you', 'on', \"'s\", 'with', '-rrb-', '-lrb-', 'as']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UEUSeWqxhygs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ayNaEb5htpc2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.5. Comparison of the 4 word lists"
      ]
    },
    {
      "metadata": {
        "id": "p4F01j4DgooL",
        "colab_type": "code",
        "outputId": "aacb3a3e-3b9a-46a2-c452-a1640b02aee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "print(pd.DataFrame.from_dict(twentyFreqWords))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   40:CoNLL15 75:OilAndGas 82:CommonCrawl\n",
            "0        </s>          lrb            the\n",
            "1           ,          rrb              ,\n",
            "2         the     sediment              .\n",
            "3           .        fault            and\n",
            "4          of        datum             to\n",
            "5         and        basin             of\n",
            "6          to       sample              a\n",
            "7           a         area             in\n",
            "8          in        study             is\n",
            "9           -        model           that\n",
            "10          )       result              i\n",
            "11          (         zone            for\n",
            "12          :        water             it\n",
            "13        for         rock            you\n",
            "14         is         time             on\n",
            "15          \"    formation             's\n",
            "16         on         high           with\n",
            "17          i      surface          -rrb-\n",
            "18       that     increase          -lrb-\n",
            "19       with       change             as\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fq8H-IL7uGdd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###1.5.1 Conclusions"
      ]
    },
    {
      "metadata": {
        "id": "DToa6OhsuWSI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Comparing the 4 lists I conclude the following:\n",
        "\n",
        "-  CoNLL15 DataSet is composed exclusively by stop words.\n",
        "-  OilAndGas DataSet contains clean pre-processed data. All stop words are removed.\n",
        "-  CommonCrawl DataSet is composed exclusively by stop words.\n",
        "-  WikiText DataSet contains clean pre-processed data. All stop words are removed."
      ]
    },
    {
      "metadata": {
        "id": "KZeoyVxluY0C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CY1neSNcud0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.6. Project top 1000 words from the WikiText corpus in 2d space using t-SNE plot"
      ]
    },
    {
      "metadata": {
        "id": "fIs9lDUowAJ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I use the model trained at stage 1.4"
      ]
    },
    {
      "metadata": {
        "id": "bBZL60xDgoXj",
        "colab_type": "code",
        "outputId": "e5a66829-5613-4f2d-cc2c-0bc9757bf9f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Get the 1000 most frequent words from the model\n",
        "mfw1000 = model.wv.index2word[:1000]\n",
        "print(mfw1000[:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['unk', 'first', 'one', 'also', 'two', 'new', 'time', 'would', 'game', 'later', 'three', 'film', 'may', 'year', 'made', 'second', 'season', 'years', 'world', 'war']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N2bZ-N1CgoHN",
        "colab_type": "code",
        "outputId": "b153cadd-5e57-42d8-b589-e1577a75ccad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        }
      },
      "cell_type": "code",
      "source": [
        "# Get the vector representation of each word in the list\n",
        "mfw1000_vecs = model[mfw1000]\n",
        "print(mfw1000_vecs[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.8655671e-03  4.5412979e-03 -3.7377772e-03  8.4503921e-04\n",
            " -4.6012774e-03  4.7684074e-03 -5.4040161e-04  2.5904330e-03\n",
            "  1.4480082e-03 -4.4996226e-03 -2.7799828e-03 -2.7477788e-03\n",
            "  1.6728049e-03 -4.9593090e-03  2.3156838e-04  4.6505779e-03\n",
            "  2.6402522e-05 -2.0656511e-03 -1.7831454e-03 -2.6781848e-03\n",
            " -2.8692437e-03 -9.9430233e-04  1.1304878e-03 -1.2564651e-03\n",
            "  4.1854358e-03 -3.8195304e-03 -2.4816170e-03  4.6105557e-03\n",
            " -7.4935844e-04 -1.8244198e-03 -1.1363826e-03  4.4023115e-03\n",
            " -3.2536786e-03 -2.3927144e-04  2.2385111e-03 -7.0144032e-04\n",
            " -5.1844853e-04 -1.7208519e-03 -8.5241324e-04  2.4389627e-03\n",
            "  4.2723813e-03  1.4387226e-03  3.4389631e-03 -4.2178594e-03\n",
            " -1.7981980e-03  2.6706930e-03  5.0389330e-04  3.2826872e-03\n",
            "  3.0838910e-03 -1.9076788e-04  3.3433323e-03 -1.8412486e-03\n",
            "  2.4158449e-03  1.1767549e-03  6.1219494e-04 -1.4081600e-03\n",
            " -8.5972471e-04 -4.6416526e-03 -4.3288963e-03  2.1201104e-03\n",
            "  3.8737925e-03 -5.9332506e-04  3.8541672e-03  3.1657084e-03\n",
            "  1.3826588e-03 -3.2183420e-04 -4.1421046e-03 -2.2805845e-03\n",
            "  4.4758394e-04 -4.3845680e-03  1.3155479e-03  9.2463661e-04\n",
            "  1.3572138e-03  1.6894488e-03 -6.1333220e-04  2.3087081e-03\n",
            "  3.4793671e-03 -3.0069370e-03  4.8817317e-03  1.7349191e-03\n",
            "  3.1290792e-03  1.7718117e-03  6.7316483e-05 -1.2104200e-03\n",
            " -3.5663747e-04  2.0694037e-03 -6.9013878e-04 -3.8348055e-03\n",
            " -3.4628515e-03 -4.1469894e-03 -2.7647521e-04  3.7318156e-03\n",
            " -3.3953940e-03  2.6759005e-03  1.7838300e-03 -3.9024001e-03\n",
            "  2.5194297e-03 -2.0756088e-03  2.7188638e-04  4.7964677e-03]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "szvibxfJgn0J",
        "colab_type": "code",
        "outputId": "9f6e121d-07d4-409d-dabb-cba4c02cb88e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "cell_type": "code",
      "source": [
        "from bokeh.models import ColumnDataSource, LabelSet\n",
        "from bokeh.plotting import figure, show, output_file\n",
        "from sklearn.manifold import TSNE\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "mfw1000_vecs_tsne = tsne.fit_transform(mfw1000_vecs)\n",
        "\n",
        "p = figure(tools=\"pan,wheel_zoom,box_zoom,reset,save\",\n",
        "           toolbar_location=\"above\",\n",
        "           title=\"Word2Vec T-SNE for the 1000 most common words\",\n",
        "           plot_width=800)\n",
        "\n",
        "source = ColumnDataSource(data=dict(x1=mfw1000_vecs_tsne[:,0],\n",
        "                                    x2=mfw1000_vecs_tsne[:,1],\n",
        "                                    names=mfw1000))\n",
        "\n",
        "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n",
        "\n",
        "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
        "                  text_font_size=\"7pt\", text_color=\"#555555\",\n",
        "                  source=source, text_align='center')\n",
        "p.add_layout(labels)\n",
        "\n",
        "show(p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div class=\"bk-root\">\n",
              "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
              "        <span id=\"1001\">Loading BokehJS ...</span>\n",
              "    </div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "\n",
              "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "  var JS_MIME_TYPE = 'application/javascript';\n",
              "  var HTML_MIME_TYPE = 'text/html';\n",
              "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
              "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
              "\n",
              "  /**\n",
              "   * Render data to the DOM node\n",
              "   */\n",
              "  function render(props, node) {\n",
              "    var script = document.createElement(\"script\");\n",
              "    node.appendChild(script);\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when an output is cleared or removed\n",
              "   */\n",
              "  function handleClearOutput(event, handle) {\n",
              "    var cell = handle.cell;\n",
              "\n",
              "    var id = cell.output_area._bokeh_element_id;\n",
              "    var server_id = cell.output_area._bokeh_server_id;\n",
              "    // Clean up Bokeh references\n",
              "    if (id != null && id in Bokeh.index) {\n",
              "      Bokeh.index[id].model.document.clear();\n",
              "      delete Bokeh.index[id];\n",
              "    }\n",
              "\n",
              "    if (server_id !== undefined) {\n",
              "      // Clean up Bokeh references\n",
              "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
              "      cell.notebook.kernel.execute(cmd, {\n",
              "        iopub: {\n",
              "          output: function(msg) {\n",
              "            var id = msg.content.text.trim();\n",
              "            if (id in Bokeh.index) {\n",
              "              Bokeh.index[id].model.document.clear();\n",
              "              delete Bokeh.index[id];\n",
              "            }\n",
              "          }\n",
              "        }\n",
              "      });\n",
              "      // Destroy server and session\n",
              "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
              "      cell.notebook.kernel.execute(cmd);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when a new output is added\n",
              "   */\n",
              "  function handleAddOutput(event, handle) {\n",
              "    var output_area = handle.output_area;\n",
              "    var output = handle.output;\n",
              "\n",
              "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
              "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "      return\n",
              "    }\n",
              "\n",
              "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
              "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
              "      // store reference to embed id on output_area\n",
              "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "    }\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "      var bk_div = document.createElement(\"div\");\n",
              "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "      var script_attrs = bk_div.children[0].attributes;\n",
              "      for (var i = 0; i < script_attrs.length; i++) {\n",
              "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "      }\n",
              "      // store reference to server id on output_area\n",
              "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function register_renderer(events, OutputArea) {\n",
              "\n",
              "    function append_mime(data, metadata, element) {\n",
              "      // create a DOM node to render to\n",
              "      var toinsert = this.create_output_subarea(\n",
              "        metadata,\n",
              "        CLASS_NAME,\n",
              "        EXEC_MIME_TYPE\n",
              "      );\n",
              "      this.keyboard_manager.register_events(toinsert);\n",
              "      // Render to node\n",
              "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "      render(props, toinsert[toinsert.length - 1]);\n",
              "      element.append(toinsert);\n",
              "      return toinsert\n",
              "    }\n",
              "\n",
              "    /* Handle when an output is cleared or removed */\n",
              "    events.on('clear_output.CodeCell', handleClearOutput);\n",
              "    events.on('delete.Cell', handleClearOutput);\n",
              "\n",
              "    /* Handle when a new output is added */\n",
              "    events.on('output_added.OutputArea', handleAddOutput);\n",
              "\n",
              "    /**\n",
              "     * Register the mime type and append_mime function with output_area\n",
              "     */\n",
              "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "      /* Is output safe? */\n",
              "      safe: true,\n",
              "      /* Index of renderer in `output_area.display_order` */\n",
              "      index: 0\n",
              "    });\n",
              "  }\n",
              "\n",
              "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
              "  if (root.Jupyter !== undefined) {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  \n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
              "     \"<div style='background-color: #fdd'>\\n\"+\n",
              "     \"<p>\\n\"+\n",
              "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
              "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
              "     \"</p>\\n\"+\n",
              "     \"<ul>\\n\"+\n",
              "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
              "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
              "     \"</ul>\\n\"+\n",
              "     \"<code>\\n\"+\n",
              "     \"from bokeh.resources import INLINE\\n\"+\n",
              "     \"output_notebook(resources=INLINE)\\n\"+\n",
              "     \"</code>\\n\"+\n",
              "     \"</div>\"}};\n",
              "\n",
              "  function display_loaded() {\n",
              "    var el = document.getElementById(\"1001\");\n",
              "    if (el != null) {\n",
              "      el.textContent = \"BokehJS is loading...\";\n",
              "    }\n",
              "    if (root.Bokeh !== undefined) {\n",
              "      if (el != null) {\n",
              "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(display_loaded, 100)\n",
              "    }\n",
              "  }\n",
              "\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
              "    }\n",
              "    finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.info(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(js_urls, callback) {\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls == null || js_urls.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    root._bokeh_is_loading = js_urls.length;\n",
              "    for (var i = 0; i < js_urls.length; i++) {\n",
              "      var url = js_urls[i];\n",
              "      var s = document.createElement('script');\n",
              "      s.src = url;\n",
              "      s.async = false;\n",
              "      s.onreadystatechange = s.onload = function() {\n",
              "        root._bokeh_is_loading--;\n",
              "        if (root._bokeh_is_loading === 0) {\n",
              "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
              "          run_callbacks()\n",
              "        }\n",
              "      };\n",
              "      s.onerror = function() {\n",
              "        console.warn(\"failed to load library \" + url);\n",
              "      };\n",
              "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "    }\n",
              "  };var element = document.getElementById(\"1001\");\n",
              "  if (element == null) {\n",
              "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
              "    return false;\n",
              "  }\n",
              "\n",
              "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.3.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.3.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.3.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.3.min.js\"];\n",
              "\n",
              "  var inline_js = [\n",
              "    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "    \n",
              "    function(Bokeh) {\n",
              "      \n",
              "    },\n",
              "    function(Bokeh) {\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.3.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.3.min.css\");\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.3.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.3.min.css\");\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.3.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.3.min.css\");\n",
              "    }\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    \n",
              "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
              "      for (var i = 0; i < inline_js.length; i++) {\n",
              "        inline_js[i].call(root, root.Bokeh);\n",
              "      }if (force === true) {\n",
              "        display_loaded();\n",
              "      }} else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    } else if (force !== true) {\n",
              "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
              "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
              "    }\n",
              "\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(js_urls, function() {\n",
              "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.3.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.3.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.3.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.3.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.3.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.3.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.3.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.3.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.3.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.3.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  <div class=\"bk-root\" id=\"96502e37-3bd4-43e5-95dd-8810efc9dfd9\" data-root-id=\"1003\"></div>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function embed_document(root) {\n",
              "    \n",
              "  var docs_json = {\"16145f1d-1fb1-4613-9ded-ed77b564b898\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1013\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"1018\",\"type\":\"LinearAxis\"}],\"plot_width\":800,\"renderers\":[{\"id\":\"1013\",\"type\":\"LinearAxis\"},{\"id\":\"1017\",\"type\":\"Grid\"},{\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"id\":\"1022\",\"type\":\"Grid\"},{\"id\":\"1030\",\"type\":\"BoxAnnotation\"},{\"id\":\"1040\",\"type\":\"GlyphRenderer\"},{\"id\":\"1042\",\"type\":\"LabelSet\"}],\"title\":{\"id\":\"1002\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1028\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"1005\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1009\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1007\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1011\",\"type\":\"LinearScale\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null},\"id\":\"1005\",\"type\":\"DataRange1d\"},{\"attributes\":{\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1014\",\"type\":\"BasicTicker\"}},\"id\":\"1017\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1050\",\"type\":\"Selection\"},{\"attributes\":{\"plot\":null,\"text\":\"Word2Vec T-SNE for the 1000 most common words\"},\"id\":\"1002\",\"type\":\"Title\"},{\"attributes\":{\"callback\":null},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1047\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"PanTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"}},\"id\":\"1039\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1019\",\"type\":\"BasicTicker\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1023\",\"type\":\"PanTool\"},{\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"id\":\"1026\",\"type\":\"ResetTool\"},{\"id\":\"1027\",\"type\":\"SaveTool\"}]},\"id\":\"1028\",\"type\":\"Toolbar\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1030\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"},\"text\":{\"field\":\"names\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"#555555\"},\"text_font_size\":{\"value\":\"7pt\"},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"},\"y_offset\":{\"value\":6}},\"id\":\"1042\",\"type\":\"LabelSet\"},{\"attributes\":{\"source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"}},\"id\":\"1041\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"}},\"id\":\"1038\",\"type\":\"Scatter\"},{\"attributes\":{\"formatter\":{\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1014\",\"type\":\"BasicTicker\"}},\"id\":\"1013\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"ResetTool\"},{\"attributes\":{\"formatter\":{\"id\":\"1047\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"callback\":null,\"data\":{\"names\":[\"unk\",\"first\",\"one\",\"also\",\"two\",\"new\",\"time\",\"would\",\"game\",\"later\",\"three\",\"film\",\"may\",\"year\",\"made\",\"second\",\"season\",\"years\",\"world\",\"war\",\"000\",\"however\",\"used\",\"song\",\"series\",\"album\",\"many\",\"team\",\"city\",\"part\",\"north\",\"became\",\"number\",\"united\",\"several\",\"including\",\"four\",\"well\",\"early\",\"state\",\"south\",\"music\",\"day\",\"episode\",\"said\",\"following\",\"states\",\"known\",\"american\",\"although\",\"could\",\"work\",\"began\",\"released\",\"like\",\"called\",\"high\",\"people\",\"end\",\"million\",\"british\",\"since\",\"around\",\"long\",\"national\",\"life\",\"best\",\"found\",\"september\",\"west\",\"back\",\"along\",\"another\",\"five\",\"show\",\"took\",\"use\",\"area\",\"final\",\"group\",\"august\",\"century\",\"received\",\"october\",\"john\",\"school\",\"due\",\"line\",\"government\",\"june\",\"east\",\"single\",\"system\",\"march\",\"july\",\"home\",\"november\",\"games\",\"third\",\"general\",\"april\",\"much\",\"large\",\"set\",\"major\",\"left\",\"based\",\"family\",\"york\",\"house\",\"place\",\"company\",\"army\",\"history\",\"december\",\"road\",\"played\",\"included\",\"name\",\"king\",\"january\",\"six\",\"band\",\"character\",\"wrote\",\"according\",\"led\",\"main\",\"within\",\"last\",\"near\",\"times\",\"described\",\"men\",\"river\",\"death\",\"video\",\"air\",\"named\",\"next\",\"battle\",\"record\",\"release\",\"late\",\"still\",\"league\",\"original\",\"make\",\"way\",\"top\",\"ship\",\"route\",\"production\",\"species\",\"even\",\"great\",\"force\",\"man\",\"club\",\"small\",\"old\",\"storm\",\"february\",\"public\",\"though\",\"days\",\"built\",\"held\",\"university\",\"play\",\"among\",\"division\",\"service\",\"role\",\"book\",\"french\",\"german\",\"side\",\"members\",\"story\",\"white\",\"power\",\"match\",\"point\",\"water\",\"player\",\"often\",\"order\",\"career\",\"continued\",\"former\",\"park\",\"local\",\"black\",\"town\",\"without\",\"came\",\"despite\",\"england\",\"considered\",\"take\",\"party\",\"half\",\"island\",\"county\",\"support\",\"form\",\"songs\",\"development\",\"version\",\"moved\",\"military\",\"written\",\"period\",\"different\",\"become\",\"president\",\"english\",\"forces\",\"lost\",\"building\",\"given\",\"court\",\"london\",\"tropical\",\"country\",\"international\",\"little\",\"church\",\"performance\",\"gave\",\"never\",\"similar\",\"tour\",\"street\",\"lead\",\"attack\",\"track\",\"returned\",\"using\",\"published\",\"royal\",\"produced\",\"short\",\"went\",\"position\",\"class\",\"recorded\",\"children\",\"television\",\"stated\",\"highway\",\"aircraft\",\"land\",\"total\",\"good\",\"western\",\"ships\",\"run\",\"include\",\"live\",\"making\",\"per\",\"control\",\"father\",\"star\",\"week\",\"women\",\"fire\",\"throughout\",\"upon\",\"right\",\"love\",\"australia\",\"head\",\"less\",\"characters\",\"design\",\"central\",\"age\",\"players\",\"young\",\"instead\",\"light\",\"writing\",\"title\",\"away\",\"rock\",\"seven\",\"across\",\"james\",\"remained\",\"months\",\"red\",\"station\",\"sea\",\"died\",\"return\",\"developed\",\"ten\",\"result\",\"previous\",\"political\",\"night\",\"law\",\"william\",\"southern\",\"hurricane\",\"eight\",\"center\",\"low\",\"reported\",\"office\",\"felt\",\"areas\",\"noted\",\"various\",\"eventually\",\"america\",\"race\",\"style\",\"announced\",\"son\",\"field\",\"created\",\"reached\",\"act\",\"miles\",\"college\",\"win\",\"construction\",\"award\",\"sent\",\"japanese\",\"seen\",\"followed\",\"together\",\"performed\",\"served\",\"member\",\"full\",\"george\",\"northern\",\"range\",\"level\",\"common\",\"killed\",\"taken\",\"able\",\"campaign\",\"points\",\"works\",\"action\",\"established\",\"chart\",\"born\",\"originally\",\"success\",\"body\",\"others\",\"front\",\"appeared\",\"thought\",\"australian\",\"modern\",\"features\",\"located\",\"project\",\"post\",\"special\",\"get\",\"playing\",\"football\",\"started\",\"stage\",\"bridge\",\"addition\",\"caused\",\"critics\",\"feet\",\"rather\",\"present\",\"fourth\",\"decided\",\"behind\",\"site\",\"formed\",\"added\",\"sold\",\"heavy\",\"championship\",\"every\",\"david\",\"director\",\"see\",\"popular\",\"strong\",\"saw\",\"damage\",\"completed\",\"population\",\"almost\",\"command\",\"important\",\"open\",\"soon\",\"close\",\"fleet\",\"eastern\",\"awards\",\"initially\",\"art\",\"kingdom\",\"guns\",\"opened\",\"non\",\"case\",\"ground\",\"region\",\"leading\",\"free\",\"weeks\",\"michael\",\"coast\",\"worked\",\"france\",\"recording\",\"working\",\"mother\",\"towards\",\"hall\",\"union\",\"event\",\"scene\",\"ever\",\"sound\",\"records\",\"scored\",\"help\",\"henry\",\"training\",\"provided\",\"navy\",\"least\",\"wanted\",\"cup\",\"average\",\"brought\",\"featured\",\"either\",\"allowed\",\"put\",\"hit\",\"wife\",\"victory\",\"troops\",\"generally\",\"studio\",\"placed\",\"washington\",\"robert\",\"example\",\"runs\",\"events\",\"magazine\",\"far\",\"europe\",\"council\",\"mid\",\"joined\",\"designed\",\"hours\",\"summer\",\"squadron\",\"human\",\"list\",\"nine\",\"opening\",\"believed\",\"start\",\"replaced\",\"official\",\"japan\",\"media\",\"mph\",\"brown\",\"operation\",\"base\",\"european\",\"involved\",\"minutes\",\"earlier\",\"police\",\"rest\",\"finished\",\"society\",\"successful\",\"met\",\"possible\",\"saying\",\"crew\",\"radio\",\"shot\",\"month\",\"significant\",\"association\",\"big\",\"particularly\",\"review\",\"highest\",\"largest\",\"films\",\"test\",\"beginning\",\"process\",\"available\",\"research\",\"change\",\"forced\",\"captain\",\"taking\",\"outside\",\"cast\",\"better\",\"charles\",\"come\",\"increased\",\"turned\",\"ended\",\"must\",\"social\",\"stone\",\"section\",\"reviews\",\"real\",\"units\",\"community\",\"thomas\",\"lower\",\"passed\",\"praised\",\"business\",\"san\",\"going\",\"relationship\",\"middle\",\"additional\",\"future\",\"move\",\"appearance\",\"hill\",\"space\",\"thus\",\"goal\",\"canada\",\"enough\",\"chief\",\"ordered\",\"winds\",\"previously\",\"cross\",\"signed\",\"elements\",\"gold\",\"rights\",\"board\",\"california\",\"program\",\"staff\",\"positive\",\"paul\",\"round\",\"lake\",\"infantry\",\"parts\",\"female\",\"critical\",\"interest\",\"castle\",\"spent\",\"told\",\"give\",\"plan\",\"gun\",\"find\",\"overall\",\"lines\",\"musical\",\"type\",\"brother\",\"attempt\",\"committee\",\"directed\",\"failed\",\"money\",\"required\",\"groups\",\"news\",\"germany\",\"hand\",\"india\",\"district\",\"evidence\",\"sometimes\",\"grand\",\"prior\",\"cover\",\"entire\",\"term\",\"asked\",\"battalion\",\"election\",\"personal\",\"port\",\"reception\",\"smith\",\"press\",\"bill\",\"room\",\"whose\",\"study\",\"claimed\",\"complete\",\"already\",\"green\",\"introduced\",\"square\",\"background\",\"plot\",\"usually\",\"turn\",\"novel\",\"indian\",\"past\",\"students\",\"person\",\"governor\",\"idea\",\"education\",\"quickly\",\"limited\",\"loss\",\"score\",\"effects\",\"peter\",\"speed\",\"stars\",\"response\",\"movement\",\"issue\",\"flight\",\"mark\",\"food\",\"surface\",\"debut\",\"commander\",\"arrived\",\"richard\",\"especially\",\"minister\",\"blue\",\"services\",\"anti\",\"decision\",\"britain\",\"woman\",\"fifth\",\"village\",\"department\",\"teams\",\"shows\",\"bay\",\"structure\",\"industry\",\"operations\",\"larger\",\"writer\",\"length\",\"science\",\"queen\",\"episodes\",\"becoming\",\"earth\",\"remaining\",\"voice\",\"centre\",\"estimated\",\"material\",\"carried\",\"defeated\",\"living\",\"latter\",\"cut\",\"pressure\",\"civil\",\"car\",\"scenes\",\"commercial\",\"leaving\",\"virginia\",\"god\",\"changes\",\"running\",\"might\",\"suggested\",\"entered\",\"report\",\"nearly\",\"los\",\"percent\",\"appointed\",\"influence\",\"approximately\",\"date\",\"depression\",\"pop\",\"naval\",\"winning\",\"language\",\"shortly\",\"islands\",\"empire\",\"view\",\"finally\",\"today\",\"child\",\"self\",\"network\",\"dark\",\"tracks\",\"hot\",\"proposed\",\"leader\",\"daughter\",\"course\",\"current\",\"wide\",\"producer\",\"higher\",\"destroyed\",\"experience\",\"size\",\"spanish\",\"provide\",\"railway\",\"fact\",\"subsequently\",\"nature\",\"compared\",\"lack\",\"married\",\"brigade\",\"immediately\",\"intended\",\"soviet\",\"supported\",\"face\",\"regular\",\"agreed\",\"cost\",\"awarded\",\"natural\",\"whether\",\"contract\",\"helped\",\"museum\",\"fans\",\"manager\",\"guitar\",\"chicago\",\"planned\",\"professional\",\"related\",\"health\",\"officer\",\"chinese\",\"create\",\"upper\",\"longer\",\"soldiers\",\"fort\",\"jackson\",\"done\",\"trade\",\"edward\",\"ball\",\"buildings\",\"plans\",\"corps\",\"captured\",\"changed\",\"appear\",\"federal\",\"private\",\"friends\",\"something\",\"associated\",\"yet\",\"officers\",\"key\",\"regiment\",\"singles\",\"books\",\"effect\",\"feature\",\"remains\",\"billboard\",\"mostly\",\"johnson\",\"initial\",\"lord\",\"countries\",\"lyrics\",\"mixed\",\"really\",\"pacific\",\"probably\",\"extended\",\"minor\",\"numerous\",\"moving\",\"friend\",\"got\",\"forward\",\"information\",\"copies\",\"always\",\"location\",\"issued\",\"difficult\",\"energy\",\"includes\",\"leave\",\"atlantic\",\"christian\",\"status\",\"singer\",\"ran\",\"conference\",\"dance\",\"suffered\",\"via\",\"pass\",\"florida\",\"unit\",\"albums\",\"edition\",\"problems\",\"know\",\"dead\",\"likely\",\"meeting\",\"fight\",\"increase\",\"roman\",\"cause\",\"independent\",\"removed\",\"inspired\",\"texas\",\"pre\",\"china\",\"wind\",\"poor\",\"fell\",\"terms\",\"primary\",\"majority\",\"movie\",\"offered\",\"keep\",\"creek\",\"winter\",\"broadcast\",\"double\",\"minute\",\"goals\",\"impact\",\"standard\",\"mary\",\"valley\",\"entertainment\",\"attacks\",\"individual\",\"takes\",\"allow\",\"composed\",\"raised\",\"wall\",\"need\",\"canadian\",\"sales\",\"male\",\"damaged\",\"ranked\",\"africa\",\"hour\",\"tons\",\"artist\",\"appears\",\"daily\",\"revealed\",\"cyclone\",\"selected\",\"discovered\",\"fighting\",\"model\",\"active\",\"era\",\"russian\",\"conditions\",\"greater\",\"unable\",\"wing\",\"inside\",\"deep\",\"mission\",\"launched\",\"respectively\",\"particular\",\"capital\",\"issues\",\"needed\",\"numbers\",\"certain\",\"morning\",\"angeles\",\"smaller\",\"lived\",\"contains\",\"jack\",\"attempted\",\"shown\",\"makes\",\"deal\",\"martin\",\"whole\",\"occurred\",\"yards\",\"attacked\",\"culture\",\"sister\",\"iii\",\"hard\",\"ice\",\"fall\",\"twenty\",\"actor\",\"elected\",\"festival\",\"annual\",\"reach\",\"referred\",\"defeat\",\"horse\",\"foreign\",\"religious\",\"traditional\",\"combat\",\"market\",\"theme\",\"19th\",\"audience\",\"largely\",\"schools\",\"think\",\"resulted\",\"metal\",\"interview\",\"assigned\",\"girl\",\"covered\",\"mass\",\"listed\",\"variety\",\"collection\",\"sun\",\"heart\",\"prince\",\"peak\",\"competition\",\"commission\",\"earned\",\"complex\",\"commented\",\"expected\",\"starting\",\"congress\",\"closed\",\"attention\",\"performances\",\"direct\",\"giving\",\"hospital\",\"refused\",\"economic\",\"greatest\",\"italian\",\"word\",\"stories\"],\"x1\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"shape\":[1000]},\"x2\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"shape\":[1000]}},\"selected\":{\"id\":\"1050\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1051\",\"type\":\"UnionRenderers\"}},\"id\":\"1036\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1038\",\"type\":\"Scatter\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1039\",\"type\":\"Scatter\"},\"selection_glyph\":null,\"view\":{\"id\":\"1041\",\"type\":\"CDSView\"}},\"id\":\"1040\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"overlay\":{\"id\":\"1030\",\"type\":\"BoxAnnotation\"}},\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1022\",\"type\":\"Grid\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.3\"}};\n",
              "  var render_items = [{\"docid\":\"16145f1d-1fb1-4613-9ded-ed77b564b898\",\"roots\":{\"1003\":\"96502e37-3bd4-43e5-95dd-8810efc9dfd9\"}}];\n",
              "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "\n",
              "  }\n",
              "  if (root.Bokeh !== undefined) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (root.Bokeh !== undefined) {\n",
              "        embed_document(root);\n",
              "        clearInterval(timer);\n",
              "      }\n",
              "      attempts++;\n",
              "      if (attempts > 100) {\n",
              "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
              "        clearInterval(timer);\n",
              "      }\n",
              "    }, 10, root)\n",
              "  }\n",
              "})(window);"
            ],
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "tags": [],
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "1003"
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "C60x6EVlotep",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gS9qvVqAotVW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YXcAS5g5ossQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DeKwKgrCsGa-"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vdVKXLvusGbM"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1. Train a word embedding model on the sentence classification corpus from the UCI Machine Learning repository"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JJklXqD0sGbX"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Download Dataset"
      ]
    },
    {
      "metadata": {
        "id": "yP7r3zO4osjy",
        "colab_type": "code",
        "outputId": "51184294-8caa-4d44-c0cb-2390b5807640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import re\n",
        "from os import listdir\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "szv6EWoWosb4",
        "colab_type": "code",
        "outputId": "f67524cd-5301-4d2c-c8f2-e07718e2bed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00311/SentenceCorpus.zip\", filename=\"SentenceCorpus.zip\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('SentenceCorpus.zip', <http.client.HTTPMessage at 0x7eff301602b0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "4jmXLOpuICW0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Data Pre-Processing"
      ]
    },
    {
      "metadata": {
        "id": "SeqVIQHMIbK7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "c-ZevFVNplt0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# After execution each single line of the training documents is an element of corpus list \n",
        "corpus = []\n",
        "of_stopwords = []\n",
        "extra_words_to_remove = ['citation','number','symbol','misc','aimx','ownx','cont','base','of','','abstract','introduction']\n",
        "\n",
        "with zipfile.ZipFile('SentenceCorpus.zip', 'r') as z:\n",
        "  z.extractall()\n",
        "  # Handling the training files\n",
        "  file_list = sorted(listdir('SentenceCorpus/labeled_articles/'))\n",
        "  for file_name in file_list:\n",
        "    if file_name.endswith('1.txt'):\n",
        "      file = z.read('SentenceCorpus/labeled_articles/' + file_name).decode('utf-8')\n",
        "      corpus.extend(file.split('\\n'))\n",
        "  # The official stop words\n",
        "  file = z.read('SentenceCorpus/word_lists/stopwords.txt').decode('utf-8')\n",
        "  of_stopwords.extend(file.split('\\n')[1:-1])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9dTahGPZlLV8",
        "colab_type": "code",
        "outputId": "3488b56e-38c6-4d7e-a8d2-9712ac9d3230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1597
        }
      },
      "cell_type": "code",
      "source": [
        "file_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.DS_Store',\n",
              " 'arxiv_annotate10_7_1.txt',\n",
              " 'arxiv_annotate10_7_2.txt',\n",
              " 'arxiv_annotate10_7_3.txt',\n",
              " 'arxiv_annotate1_13_1.txt',\n",
              " 'arxiv_annotate1_13_2.txt',\n",
              " 'arxiv_annotate1_13_3.txt',\n",
              " 'arxiv_annotate2_66_1.txt',\n",
              " 'arxiv_annotate2_66_2.txt',\n",
              " 'arxiv_annotate2_66_3.txt',\n",
              " 'arxiv_annotate3_80_1.txt',\n",
              " 'arxiv_annotate3_80_2.txt',\n",
              " 'arxiv_annotate3_80_3.txt',\n",
              " 'arxiv_annotate4_168_1.txt',\n",
              " 'arxiv_annotate4_168_2.txt',\n",
              " 'arxiv_annotate4_168_3.txt',\n",
              " 'arxiv_annotate5_240_1.txt',\n",
              " 'arxiv_annotate5_240_2.txt',\n",
              " 'arxiv_annotate5_240_3.txt',\n",
              " 'arxiv_annotate6_52_1.txt',\n",
              " 'arxiv_annotate6_52_2.txt',\n",
              " 'arxiv_annotate6_52_3.txt',\n",
              " 'arxiv_annotate7_268_1.txt',\n",
              " 'arxiv_annotate7_268_2.txt',\n",
              " 'arxiv_annotate7_268_3.txt',\n",
              " 'arxiv_annotate8_81_1.txt',\n",
              " 'arxiv_annotate8_81_2.txt',\n",
              " 'arxiv_annotate8_81_3.txt',\n",
              " 'arxiv_annotate9_279_1.txt',\n",
              " 'arxiv_annotate9_279_2.txt',\n",
              " 'arxiv_annotate9_279_3.txt',\n",
              " 'jdm_annotate10_210_1.txt',\n",
              " 'jdm_annotate10_210_2.txt',\n",
              " 'jdm_annotate10_210_3.txt',\n",
              " 'jdm_annotate1_103_1.txt',\n",
              " 'jdm_annotate1_103_2.txt',\n",
              " 'jdm_annotate1_103_3.txt',\n",
              " 'jdm_annotate2_107_1.txt',\n",
              " 'jdm_annotate2_107_2.txt',\n",
              " 'jdm_annotate2_107_3.txt',\n",
              " 'jdm_annotate3_120_1.txt',\n",
              " 'jdm_annotate3_120_2.txt',\n",
              " 'jdm_annotate3_120_3.txt',\n",
              " 'jdm_annotate4_220_1.txt',\n",
              " 'jdm_annotate4_220_2.txt',\n",
              " 'jdm_annotate4_220_3.txt',\n",
              " 'jdm_annotate5_228_1.txt',\n",
              " 'jdm_annotate5_228_2.txt',\n",
              " 'jdm_annotate5_228_3.txt',\n",
              " 'jdm_annotate6_32_1.txt',\n",
              " 'jdm_annotate6_32_2.txt',\n",
              " 'jdm_annotate6_32_3.txt',\n",
              " 'jdm_annotate7_265_1.txt',\n",
              " 'jdm_annotate7_265_2.txt',\n",
              " 'jdm_annotate7_265_3.txt',\n",
              " 'jdm_annotate8_177_1.txt',\n",
              " 'jdm_annotate8_177_2.txt',\n",
              " 'jdm_annotate8_177_3.txt',\n",
              " 'jdm_annotate9_45_1.txt',\n",
              " 'jdm_annotate9_45_2.txt',\n",
              " 'jdm_annotate9_45_3.txt',\n",
              " 'plos_annotate10_1140_1.txt',\n",
              " 'plos_annotate10_1140_2.txt',\n",
              " 'plos_annotate10_1140_3.txt',\n",
              " 'plos_annotate1_6_1.txt',\n",
              " 'plos_annotate1_6_2.txt',\n",
              " 'plos_annotate1_6_3.txt',\n",
              " 'plos_annotate2_336_1.txt',\n",
              " 'plos_annotate2_336_2.txt',\n",
              " 'plos_annotate2_336_3.txt',\n",
              " 'plos_annotate3_798_1.txt',\n",
              " 'plos_annotate3_798_2.txt',\n",
              " 'plos_annotate3_798_3.txt',\n",
              " 'plos_annotate4_1052_1.txt',\n",
              " 'plos_annotate4_1052_2.txt',\n",
              " 'plos_annotate4_1052_3.txt',\n",
              " 'plos_annotate5_1375_1.txt',\n",
              " 'plos_annotate5_1375_2.txt',\n",
              " 'plos_annotate5_1375_3.txt',\n",
              " 'plos_annotate6_1032_1.txt',\n",
              " 'plos_annotate6_1032_2.txt',\n",
              " 'plos_annotate6_1032_3.txt',\n",
              " 'plos_annotate7_1233_1.txt',\n",
              " 'plos_annotate7_1233_2.txt',\n",
              " 'plos_annotate7_1233_3.txt',\n",
              " 'plos_annotate8_123_1.txt',\n",
              " 'plos_annotate8_123_2.txt',\n",
              " 'plos_annotate8_123_3.txt',\n",
              " 'plos_annotate9_1187_1.txt',\n",
              " 'plos_annotate9_1187_2.txt',\n",
              " 'plos_annotate9_1187_3.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "hnvvEWTbSITP",
        "colab_type": "code",
        "outputId": "ea722d52-2f4f-4557-a95c-1497f4cd70f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "corpus[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MISC\\tThe Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied\\r'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "metadata": {
        "id": "kMrDXe45Ny3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# After execution each element of corpus list (line of the training documents) is\n",
        "# transformed to a list of clean tokens as element in preproc_corpus list\n",
        "preproc_corpus = []\n",
        "for doc in corpus:\n",
        "  doc = doc.lower()\n",
        "  doc = re.sub(r'[^a-z0-9]+', ' ',doc)\n",
        "  doc = re.sub(r'\\s+', ' ',doc)\n",
        "  doc = doc.split(' ')\n",
        "  doc = [word for word in doc if word not in of_stopwords+extra_words_to_remove+stopwords.words('english')]\n",
        "  doc = [word for word in doc if len(word)>1]\n",
        "  if len(doc) > 0:\n",
        "    preproc_corpus.append(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0xPr_7ASoji",
        "colab_type": "code",
        "outputId": "bdaf8a6d-b514-4c4d-d48d-3335d4487e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "preproc_corpus[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['minimum',\n",
              " 'description',\n",
              " 'length',\n",
              " 'principle',\n",
              " 'online',\n",
              " 'sequence',\n",
              " 'estimation',\n",
              " 'prediction',\n",
              " 'proper',\n",
              " 'learning',\n",
              " 'setup',\n",
              " 'studied']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "metadata": {
        "id": "3aXbP-PIU2xX",
        "colab_type": "code",
        "outputId": "38062b80-5bfe-4163-c8f2-47922242d294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(corpus), len(preproc_corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1120, 1039)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "metadata": {
        "id": "e0spAVGaVKRF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.3 Train a basic word embedding model"
      ]
    },
    {
      "metadata": {
        "id": "p2i3vIZYKPYQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def customW2Vmodel(window=4,size=100,sg=1,min_count=1,workers=-1,negative=5):\n",
        "  model = Word2Vec(window=window,size=size,sg=sg,min_count=min_count,workers=workers,negative=negative)\n",
        "  model.build_vocab(preproc_corpus)  # Building the model vocabulary\n",
        "  model.train(preproc_corpus,total_examples=model.corpus_count,epochs=model.iter)\n",
        "  return(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zXhQK8QaLMWL",
        "colab_type": "code",
        "outputId": "41c8c254-4a70-43b4-a29b-15c9d381d81e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model0 = customW2Vmodel(window=4,size=100,sg=1,min_count=1,workers=-1,negative=5)\n",
        "print('Vocabulary size: ',len(model0.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 99.3 ms, sys: 1.87 ms, total: 101 ms\n",
            "Wall time: 107 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "MPha83k5ekbA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.2 Train other models on the same data, but with one hyperparameter different (for example, window size or vector size)"
      ]
    },
    {
      "metadata": {
        "id": "U9OIM16Ny2Iu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Adjusting vector dimensionality (size)"
      ]
    },
    {
      "metadata": {
        "id": "5N7-kdUWU2Re",
        "colab_type": "code",
        "outputId": "818eaf44-f91e-4156-bbcd-cda04935ad63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model1 = customW2Vmodel(size=250)\n",
        "print('Vocabulary size: ',len(model1.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 97.8 ms, sys: 1.92 ms, total: 99.7 ms\n",
            "Wall time: 106 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "a9a9e99a-e41b-438a-cd39-cbcf2810a5cb",
        "id": "nq66UhnA7b4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model11 = customW2Vmodel(size=500)\n",
        "print('Vocabulary size: ',len(model1.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 114 ms, sys: 2.94 ms, total: 117 ms\n",
            "Wall time: 123 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "48Q61a2S0v7j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Adjusting window size"
      ]
    },
    {
      "metadata": {
        "id": "6GgRoCSxU2EP",
        "colab_type": "code",
        "outputId": "c2ab5e67-26e5-4e57-cebc-4baba4bc1580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model2 = customW2Vmodel(window=8)\n",
        "print('Vocabulary size: ',len(model2.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 91.5 ms, sys: 5.53 ms, total: 97 ms\n",
            "Wall time: 100 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YPeLfydZ1MEx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.3 Adjusting minimum token frequency (min_count)"
      ]
    },
    {
      "metadata": {
        "id": "sHgCpSmH1NB6",
        "colab_type": "code",
        "outputId": "666affa1-89b1-4e0a-d17e-668e5ce7a514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model3 = customW2Vmodel(min_count=5)\n",
        "print('Vocabulary size: ',len(model3.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  870\n",
            "CPU times: user 39.4 ms, sys: 3.04 ms, total: 42.4 ms\n",
            "Wall time: 46.9 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "hrfhJFirGzOK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.4 Adjusting negative sampling"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "da4fc75a-66aa-4ad9-c6f4-518bb2e8cae2",
        "id": "BLbY6hFoGwp5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model4 = customW2Vmodel(negative=10)\n",
        "print('Vocabulary size: ',len(model4.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 94.2 ms, sys: 3.78 ms, total: 98 ms\n",
            "Wall time: 102 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "_70tNxvUHpDa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.5 Adjusting training algorithm (sg=0 for CBOW; sg=1 for skip-gram )"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "3b6b0034-ca5b-4be8-8e15-50049732502e",
        "id": "zxdVzFifHnJf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model5 = customW2Vmodel(sg=0)\n",
        "print('Vocabulary size: ',len(model5.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 104 ms, sys: 1.77 ms, total: 106 ms\n",
            "Wall time: 110 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xs0koOPbPAzV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QRZkQfe5w0TP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3. Compare models with respect to Performance\n",
        "\n",
        "I choose a set of metrics to compare models' performance, including training time and results delivering from various in-built functions"
      ]
    },
    {
      "metadata": {
        "id": "88rONMZaLDfb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model to compare\n",
        "models = [('Basic Model',model0),('Vector Dimensionality 250',model1),\n",
        "          ('Vector Dimensionality 500',model11),('Window Size',model2),\n",
        "          ('Min Token Frequency',model3),('Negative Sampling',model4),\n",
        "          ('Train Algorithm',model5)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wl3G2KcqXoiq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.1 Training time"
      ]
    },
    {
      "metadata": {
        "id": "k-dgceTUYQp0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "ZcpWrsAvYSdD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.2 10 best pairs of words"
      ]
    },
    {
      "metadata": {
        "id": "CWaa5ZxW4IeF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m_hDRsmz-B7Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function that uses the model to determine the N best pairs of tokens, based on\n",
        "# similarity, among all possible pairs in the vocabolary\n",
        "def bestSimPairs(model,N,vocab=None):\n",
        "  import operator\n",
        "  \n",
        "  # Use dictionary structure to store word pairs and their similarities\n",
        "  similar_pairs = {}\n",
        "  if(vocab == None):\n",
        "    vocab = model.wv.index2word\n",
        "    \n",
        "  # Compare each word of the vocabulary against all others and find similarities\n",
        "  for p1 in vocab:\n",
        "    pairs = model.wv.most_similar(p1,topn=1)\n",
        "    p2,sim = pairs[0]\n",
        "    similar_pairs[(p1, p2)] = sim\n",
        "\n",
        "  # Sort dictionary's data by values\n",
        "  sorted_similar_pairs = sorted(similar_pairs.items(),key=operator.itemgetter(1),reverse=True)\n",
        "\n",
        "  # Since every pair appears twice, I keep every other element of the ordered list\n",
        "  sorted_similar_pairs = sorted_similar_pairs[0:2*N:2]\n",
        "  \n",
        "  return(sorted_similar_pairs) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PgKxMCFyb-wy",
        "outputId": "ea4b091a-e4d2-437d-82d2-3ca1df3d4160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1513
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = None\n",
        "\n",
        "best_pairs_by_method = {}\n",
        "\n",
        "for (name,model) in models:\n",
        "  best_pairs_by_method[name] = bestSimPairs(model, 10, vocab)\n",
        "\n",
        "print(pd.DataFrame.from_dict(best_pairs_by_method))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                      Basic Model  \\\n",
            "0          ((noisy, driving), 0.4894424080848694)   \n",
            "1      ((works, assemblies), 0.48557597398757935)   \n",
            "2         ((simpler, burst), 0.48217469453811646)   \n",
            "3  ((inserts, contribution), 0.47997167706489563)   \n",
            "4              ((deal, care), 0.4799291491508484)   \n",
            "5   ((employ, concurrently), 0.46593475341796875)   \n",
            "6              ((go, halali), 0.4637524485588074)   \n",
            "7         ((fit, employing), 0.45622801780700684)   \n",
            "8  ((specifically, maximize), 0.4561006724834442)   \n",
            "9             ((isps, odour), 0.4546169638633728)   \n",
            "\n",
            "                                 Min Token Frequency  \\\n",
            "0            ((variants, times), 0.4278267025947571)   \n",
            "1  ((explicitly, psychological), 0.4277902841567993)   \n",
            "2             ((new, involving), 0.4151037931442261)   \n",
            "3              ((human, hetero), 0.4103476405143738)   \n",
            "4           ((believed, times), 0.39652907848358154)   \n",
            "5          ((processing, show), 0.39632487297058105)   \n",
            "6    ((generally, constraints), 0.39540350437164307)   \n",
            "7               ((input, usage), 0.3948977291584015)   \n",
            "8               ((maker, shown), 0.3939894437789917)   \n",
            "9               ((favor, impact), 0.393761545419693)   \n",
            "\n",
            "                                Negative Sampling  \\\n",
            "0          ((noisy, driving), 0.4894424080848694)   \n",
            "1      ((works, assemblies), 0.48557597398757935)   \n",
            "2         ((simpler, burst), 0.48217469453811646)   \n",
            "3  ((inserts, contribution), 0.47997167706489563)   \n",
            "4              ((deal, care), 0.4799291491508484)   \n",
            "5   ((employ, concurrently), 0.46593475341796875)   \n",
            "6              ((go, halali), 0.4637524485588074)   \n",
            "7         ((fit, employing), 0.45622801780700684)   \n",
            "8  ((specifically, maximize), 0.4561006724834442)   \n",
            "9             ((isps, odour), 0.4546169638633728)   \n",
            "\n",
            "                                  Train Algorithm  \\\n",
            "0          ((noisy, driving), 0.4894424080848694)   \n",
            "1      ((works, assemblies), 0.48557597398757935)   \n",
            "2         ((simpler, burst), 0.48217469453811646)   \n",
            "3  ((inserts, contribution), 0.47997167706489563)   \n",
            "4              ((deal, care), 0.4799291491508484)   \n",
            "5   ((employ, concurrently), 0.46593475341796875)   \n",
            "6              ((go, halali), 0.4637524485588074)   \n",
            "7         ((fit, employing), 0.45622801780700684)   \n",
            "8  ((specifically, maximize), 0.4561006724834442)   \n",
            "9             ((isps, odour), 0.4546169638633728)   \n",
            "\n",
            "                       Vector Dimensionality 250  \\\n",
            "0  ((dynamically, pioneered), 0.342629075050354)   \n",
            "1        ((desire, humans), 0.32499808073043823)   \n",
            "2            ((target, lag), 0.3200412690639496)   \n",
            "3    ((preference, search), 0.31591999530792236)   \n",
            "4     ((maximizing, actual), 0.3146668076515198)   \n",
            "5    ((skipping, currently), 0.3127231001853943)   \n",
            "6        ((backbone, reals), 0.3062945604324341)   \n",
            "7       ((progressed, sce), 0.30408334732055664)   \n",
            "8    ((however, hindering), 0.30358678102493286)   \n",
            "9    ((serving, criticized), 0.3035593628883362)   \n",
            "\n",
            "                         Vector Dimensionality 500  \\\n",
            "0     ((skipping, currently), 0.25298449397087097)   \n",
            "1        ((entirely, select), 0.24366194009780884)   \n",
            "2  ((feedback, prerequisite), 0.22531679272651672)   \n",
            "3      ((genes, appropriate), 0.22392675280570984)   \n",
            "4              ((owned, etc), 0.22205279767513275)   \n",
            "5               ((search, vc), 0.2196393609046936)   \n",
            "6              ((rule, worth), 0.2168198823928833)   \n",
            "7           ((repress, curry), 0.2161499559879303)   \n",
            "8   ((presented, unfavorable), 0.2155771255493164)   \n",
            "9           ((effective, 50), 0.21528270840644836)   \n",
            "\n",
            "                                      Window Size  \n",
            "0          ((noisy, driving), 0.4894424080848694)  \n",
            "1      ((works, assemblies), 0.48557597398757935)  \n",
            "2         ((simpler, burst), 0.48217469453811646)  \n",
            "3  ((inserts, contribution), 0.47997167706489563)  \n",
            "4              ((deal, care), 0.4799291491508484)  \n",
            "5   ((employ, concurrently), 0.46593475341796875)  \n",
            "6              ((go, halali), 0.4637524485588074)  \n",
            "7         ((fit, employing), 0.45622801780700684)  \n",
            "8  ((specifically, maximize), 0.4561006724834442)  \n",
            "9             ((isps, odour), 0.4546169638633728)  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qrccPKtCZln7",
        "colab_type": "code",
        "outputId": "2f4b9d4e-629c-4207-87bc-9d1fbd2bad0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1513
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = model3.wv.index2word\n",
        "\n",
        "best_pairs_by_method = {}\n",
        "\n",
        "for (name,model) in models:\n",
        "  best_pairs_by_method[name] = bestSimPairs(model, 10, vocab)\n",
        "\n",
        "print(pd.DataFrame.from_dict(best_pairs_by_method))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                         Basic Model  \\\n",
            "0         ((works, assemblies), 0.48557597398757935)   \n",
            "1     ((specifically, maximize), 0.4561006724834442)   \n",
            "2    ((alternative, identifies), 0.4453243613243103)   \n",
            "3     ((direction, predicting), 0.44045907258987427)   \n",
            "4              ((probably, 300), 0.4356062412261963)   \n",
            "5             ((measure, nervous), 0.43490070104599)   \n",
            "6               ((three, lbfr), 0.43085259199142456)   \n",
            "7            ((times, variants), 0.4278267025947571)   \n",
            "8  ((psychological, explicitly), 0.4277902841567993)   \n",
            "9        ((required, regimens), 0.42751410603523254)   \n",
            "\n",
            "                                 Min Token Frequency  \\\n",
            "0            ((variants, times), 0.4278267025947571)   \n",
            "1  ((explicitly, psychological), 0.4277902841567993)   \n",
            "2             ((new, involving), 0.4151037931442261)   \n",
            "3              ((human, hetero), 0.4103476405143738)   \n",
            "4           ((believed, times), 0.39652907848358154)   \n",
            "5          ((processing, show), 0.39632487297058105)   \n",
            "6    ((generally, constraints), 0.39540350437164307)   \n",
            "7               ((input, usage), 0.3948977291584015)   \n",
            "8               ((maker, shown), 0.3939894437789917)   \n",
            "9               ((favor, impact), 0.393761545419693)   \n",
            "\n",
            "                                   Negative Sampling  \\\n",
            "0         ((works, assemblies), 0.48557597398757935)   \n",
            "1     ((specifically, maximize), 0.4561006724834442)   \n",
            "2    ((alternative, identifies), 0.4453243613243103)   \n",
            "3     ((direction, predicting), 0.44045907258987427)   \n",
            "4              ((probably, 300), 0.4356062412261963)   \n",
            "5             ((measure, nervous), 0.43490070104599)   \n",
            "6               ((three, lbfr), 0.43085259199142456)   \n",
            "7            ((times, variants), 0.4278267025947571)   \n",
            "8  ((psychological, explicitly), 0.4277902841567993)   \n",
            "9        ((required, regimens), 0.42751410603523254)   \n",
            "\n",
            "                                     Train Algorithm  \\\n",
            "0         ((works, assemblies), 0.48557597398757935)   \n",
            "1     ((specifically, maximize), 0.4561006724834442)   \n",
            "2    ((alternative, identifies), 0.4453243613243103)   \n",
            "3     ((direction, predicting), 0.44045907258987427)   \n",
            "4              ((probably, 300), 0.4356062412261963)   \n",
            "5             ((measure, nervous), 0.43490070104599)   \n",
            "6               ((three, lbfr), 0.43085259199142456)   \n",
            "7            ((times, variants), 0.4278267025947571)   \n",
            "8  ((psychological, explicitly), 0.4277902841567993)   \n",
            "9        ((required, regimens), 0.42751410603523254)   \n",
            "\n",
            "                      Vector Dimensionality 250  \\\n",
            "0           ((target, lag), 0.3200412690639496)   \n",
            "1       ((backbone, reals), 0.3062945604324341)   \n",
            "2   ((however, hindering), 0.30358678102493286)   \n",
            "3   ((genes, appropriate), 0.29073572158813477)   \n",
            "4        ((suggests, note), 0.2878541648387909)   \n",
            "5          ((seen, matrix), 0.2875481843948364)   \n",
            "6     ((influenced, lived), 0.2864241600036621)   \n",
            "7              ((yet, go), 0.28117358684539795)   \n",
            "8      ((respect, partial), 0.2800878584384918)   \n",
            "9  ((dependent, possible), 0.28007036447525024)   \n",
            "\n",
            "                         Vector Dimensionality 500  \\\n",
            "0  ((feedback, prerequisite), 0.22531679272651672)   \n",
            "1              ((rule, worth), 0.2168198823928833)   \n",
            "2           ((effective, 50), 0.21528270840644836)   \n",
            "3        ((complex, methods), 0.21153435111045837)   \n",
            "4          ((bias, knockout), 0.20885895192623138)   \n",
            "5      ((paradox, validated), 0.20330971479415894)   \n",
            "6            ((suggests, go), 0.20004470646381378)   \n",
            "7      ((subject, compressed), 0.1985711008310318)   \n",
            "8          ((option, mining), 0.19611331820487976)   \n",
            "9      ((literature, private), 0.1942547708749771)   \n",
            "\n",
            "                                         Window Size  \n",
            "0         ((works, assemblies), 0.48557597398757935)  \n",
            "1     ((specifically, maximize), 0.4561006724834442)  \n",
            "2    ((alternative, identifies), 0.4453243613243103)  \n",
            "3     ((direction, predicting), 0.44045907258987427)  \n",
            "4              ((probably, 300), 0.4356062412261963)  \n",
            "5             ((measure, nervous), 0.43490070104599)  \n",
            "6               ((three, lbfr), 0.43085259199142456)  \n",
            "7            ((times, variants), 0.4278267025947571)  \n",
            "8  ((psychological, explicitly), 0.4277902841567993)  \n",
            "9        ((required, regimens), 0.42751410603523254)  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r6Xo4ecmBsuY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.3 Compute words' cosine distance"
      ]
    },
    {
      "metadata": {
        "id": "qkTrJ_17Tfv-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Find the cosine distance of pairs of words"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CcxK2fKyT-WF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function that accepts a trained word2vec model and list of pairs of words and\n",
        "# computes the cosine distance of each pair\n",
        "def cosDist(model, pairs):\n",
        "  cos_dist = []\n",
        "  \n",
        "  for pair in pairs:\n",
        "    w1,w2 = pair\n",
        "    cos_dist.append(model.wv.similarity(w1,w2))\n",
        "    \n",
        "  return(cos_dist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kKLTeUb8T-W3",
        "outputId": "4139c50f-d47a-45fd-9fc9-9a177d23232f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Create 5 random pairs of words from the smallest vocabulary\n",
        "vocab = model3.wv.index2word\n",
        "list = []\n",
        "for i in range(5):\n",
        "  list.append(random.choices(vocab, k=2))\n",
        "\n",
        "cosDist_by_method = {}\n",
        "cosDist_by_method['List of pairs'] = list\n",
        "\n",
        "# Use the methods to determine the cosine distance of words in each pair\n",
        "for (name,model) in models:\n",
        "  cosDist_by_method[name] = cosDist(model, list)\n",
        "\n",
        "print(pd.DataFrame.from_dict(cosDist_by_method).set_index('List of pairs'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       Basic Model  Min Token Frequency  Negative Sampling  \\\n",
            "List of pairs                                                                \n",
            "[exchange, easy]         -0.004057            -0.004057          -0.004057   \n",
            "[stimuli, complexity]    -0.035405            -0.035405          -0.035405   \n",
            "[responsible, easy]      -0.038228            -0.038228          -0.038228   \n",
            "[implies, could]          0.135873             0.135873           0.135873   \n",
            "[recent, correct]         0.156587             0.156587           0.156587   \n",
            "\n",
            "                       Train Algorithm  Vector Dimensionality 250  \\\n",
            "List of pairs                                                       \n",
            "[exchange, easy]             -0.004057                   0.079440   \n",
            "[stimuli, complexity]        -0.035405                  -0.130871   \n",
            "[responsible, easy]          -0.038228                   0.022298   \n",
            "[implies, could]              0.135873                   0.104804   \n",
            "[recent, correct]             0.156587                   0.107683   \n",
            "\n",
            "                       Vector Dimensionality 500  Window Size  \n",
            "List of pairs                                                  \n",
            "[exchange, easy]                        0.004655    -0.004057  \n",
            "[stimuli, complexity]                  -0.134064    -0.035405  \n",
            "[responsible, easy]                     0.019640    -0.038228  \n",
            "[implies, could]                        0.056555     0.135873  \n",
            "[recent, correct]                      -0.003747     0.156587  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RgEXfXBCX2Kg"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.4 Compute phrases' cosine distance"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "C_k2i3o4X374"
      },
      "cell_type": "markdown",
      "source": [
        "Find the cosine distance of lists of words or strings"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OxeygS2lYLO4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function that accepts a trained model and list of pairs of lists of words or\n",
        "# strings and computes the cosine distance of each pair. Strings in a pair should\n",
        "# have same length\n",
        "def cosDistPhrase(model, lists):\n",
        "  cos_phrase_dist = []\n",
        "  \n",
        "  for pair in lists:\n",
        "    p1,p2 = pair\n",
        "    cos_phrase_dist.append(model.wv.n_similarity(p1,p2))\n",
        "    \n",
        "  return(cos_phrase_dist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bUQC_NWrYNgK",
        "outputId": "a22e7638-eedd-4e41-9325-671421081455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1027
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Create 5 pairs of random 4-element lists of words from the smallest vocabulary\n",
        "vocab = model3.wv.index2word\n",
        "lists = []\n",
        "for i in range(5):\n",
        "  lists.append([random.choices(vocab, k=4), random.choices(vocab, k=4)])\n",
        "\n",
        "cosDistPhrase_by_method = {}\n",
        "cosDistPhrase_by_method['Lists of pairs'] = lists\n",
        "\n",
        "# Use the methods to determine the cosine distance of words in each list\n",
        "for (name,model) in models:\n",
        "  cosDistPhrase_by_method[name] = cosDistPhrase(model, lists)\n",
        "\n",
        "print(pd.DataFrame.from_dict(cosDistPhrase_by_method).set_index('Lists of pairs'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    Basic Model  \\\n",
            "Lists of pairs                                                    \n",
            "[[decisions, instances, cellular, testing], [un...     0.044305   \n",
            "[[output, involving, questions, disutility], [p...    -0.087051   \n",
            "[[hard, place, age, majority], [network, belief...     0.117287   \n",
            "[[preferences, probability, value, six], [proce...    -0.095530   \n",
            "[[would, complexity, context, solution], [viewe...     0.147153   \n",
            "\n",
            "                                                    Min Token Frequency  \\\n",
            "Lists of pairs                                                            \n",
            "[[decisions, instances, cellular, testing], [un...             0.044305   \n",
            "[[output, involving, questions, disutility], [p...            -0.087051   \n",
            "[[hard, place, age, majority], [network, belief...             0.117287   \n",
            "[[preferences, probability, value, six], [proce...            -0.095530   \n",
            "[[would, complexity, context, solution], [viewe...             0.147153   \n",
            "\n",
            "                                                    Negative Sampling  \\\n",
            "Lists of pairs                                                          \n",
            "[[decisions, instances, cellular, testing], [un...           0.044305   \n",
            "[[output, involving, questions, disutility], [p...          -0.087051   \n",
            "[[hard, place, age, majority], [network, belief...           0.117287   \n",
            "[[preferences, probability, value, six], [proce...          -0.095530   \n",
            "[[would, complexity, context, solution], [viewe...           0.147153   \n",
            "\n",
            "                                                    Train Algorithm  \\\n",
            "Lists of pairs                                                        \n",
            "[[decisions, instances, cellular, testing], [un...         0.044305   \n",
            "[[output, involving, questions, disutility], [p...        -0.087051   \n",
            "[[hard, place, age, majority], [network, belief...         0.117287   \n",
            "[[preferences, probability, value, six], [proce...        -0.095530   \n",
            "[[would, complexity, context, solution], [viewe...         0.147153   \n",
            "\n",
            "                                                    Vector Dimensionality 250  \\\n",
            "Lists of pairs                                                                  \n",
            "[[decisions, instances, cellular, testing], [un...                  -0.088979   \n",
            "[[output, involving, questions, disutility], [p...                  -0.043583   \n",
            "[[hard, place, age, majority], [network, belief...                  -0.010387   \n",
            "[[preferences, probability, value, six], [proce...                  -0.098134   \n",
            "[[would, complexity, context, solution], [viewe...                   0.067941   \n",
            "\n",
            "                                                    Vector Dimensionality 500  \\\n",
            "Lists of pairs                                                                  \n",
            "[[decisions, instances, cellular, testing], [un...                  -0.059622   \n",
            "[[output, involving, questions, disutility], [p...                  -0.023088   \n",
            "[[hard, place, age, majority], [network, belief...                   0.022662   \n",
            "[[preferences, probability, value, six], [proce...                  -0.086583   \n",
            "[[would, complexity, context, solution], [viewe...                   0.042429   \n",
            "\n",
            "                                                    Window Size  \n",
            "Lists of pairs                                                   \n",
            "[[decisions, instances, cellular, testing], [un...     0.044305  \n",
            "[[output, involving, questions, disutility], [p...    -0.087051  \n",
            "[[hard, place, age, majority], [network, belief...     0.117287  \n",
            "[[preferences, probability, value, six], [proce...    -0.095530  \n",
            "[[would, complexity, context, solution], [viewe...     0.147153  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "05SlVR2qTKxO"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.5 Find which word doesn't match with the others"
      ]
    },
    {
      "metadata": {
        "id": "BkpoucYzCIQE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Which word from the given list doesn’t go with the others?"
      ]
    },
    {
      "metadata": {
        "id": "sDCjrKvXaXGf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function that accepts lists of words and uses the model to determine which word\n",
        "# doesn't match with the others\n",
        "def doesntMatch(model, lists):\n",
        "  dont_match = []\n",
        "  \n",
        "  for list in lists:\n",
        "    dont_match.append(model.doesnt_match(list))\n",
        "    \n",
        "  return(dont_match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PTjlqzdzJKZO",
        "colab_type": "code",
        "outputId": "fb2a6138-f66f-4090-d96d-1092a8ec9ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1062
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Create 5 random 5-element lists from the smallest vocabulary\n",
        "vocab = model3.wv.index2word\n",
        "lists = []\n",
        "for i in range(5):\n",
        "  lists.append(random.choices(vocab, k=5))\n",
        "\n",
        "doesnt_match_by_method = {}\n",
        "doesnt_match_by_method['Lists of words'] = lists\n",
        "\n",
        "# Use the methods to determine which word doesn't match in each list\n",
        "for (name,model) in models:\n",
        "  doesnt_match_by_method[name] = doesntMatch(model, lists)\n",
        "\n",
        "print(pd.DataFrame.from_dict(doesnt_match_by_method).set_index('Lists of words'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Basic Model  \\\n",
            "Lists of words                                                \n",
            "[polynomial, firing, length, three, fixed]            three   \n",
            "[real, consequence, involving, splicing, task]  consequence   \n",
            "[framing, dual, couple, bernoulli, option]        bernoulli   \n",
            "[speed, analyzed, variants, predict, feedback]     analyzed   \n",
            "[able, instances, couple, procedure, whether]        couple   \n",
            "\n",
            "                                               Min Token Frequency  \\\n",
            "Lists of words                                                       \n",
            "[polynomial, firing, length, three, fixed]                   three   \n",
            "[real, consequence, involving, splicing, task]         consequence   \n",
            "[framing, dual, couple, bernoulli, option]               bernoulli   \n",
            "[speed, analyzed, variants, predict, feedback]            analyzed   \n",
            "[able, instances, couple, procedure, whether]               couple   \n",
            "\n",
            "                                               Negative Sampling  \\\n",
            "Lists of words                                                     \n",
            "[polynomial, firing, length, three, fixed]                 three   \n",
            "[real, consequence, involving, splicing, task]       consequence   \n",
            "[framing, dual, couple, bernoulli, option]             bernoulli   \n",
            "[speed, analyzed, variants, predict, feedback]          analyzed   \n",
            "[able, instances, couple, procedure, whether]             couple   \n",
            "\n",
            "                                               Train Algorithm  \\\n",
            "Lists of words                                                   \n",
            "[polynomial, firing, length, three, fixed]               three   \n",
            "[real, consequence, involving, splicing, task]     consequence   \n",
            "[framing, dual, couple, bernoulli, option]           bernoulli   \n",
            "[speed, analyzed, variants, predict, feedback]        analyzed   \n",
            "[able, instances, couple, procedure, whether]           couple   \n",
            "\n",
            "                                               Vector Dimensionality 250  \\\n",
            "Lists of words                                                             \n",
            "[polynomial, firing, length, three, fixed]                         three   \n",
            "[real, consequence, involving, splicing, task]               consequence   \n",
            "[framing, dual, couple, bernoulli, option]                          dual   \n",
            "[speed, analyzed, variants, predict, feedback]                     speed   \n",
            "[able, instances, couple, procedure, whether]                     couple   \n",
            "\n",
            "                                               Vector Dimensionality 500  \\\n",
            "Lists of words                                                             \n",
            "[polynomial, firing, length, three, fixed]                         three   \n",
            "[real, consequence, involving, splicing, task]               consequence   \n",
            "[framing, dual, couple, bernoulli, option]                     bernoulli   \n",
            "[speed, analyzed, variants, predict, feedback]                   predict   \n",
            "[able, instances, couple, procedure, whether]                     couple   \n",
            "\n",
            "                                                Window Size  \n",
            "Lists of words                                               \n",
            "[polynomial, firing, length, three, fixed]            three  \n",
            "[real, consequence, involving, splicing, task]  consequence  \n",
            "[framing, dual, couple, bernoulli, option]        bernoulli  \n",
            "[speed, analyzed, variants, predict, feedback]     analyzed  \n",
            "[able, instances, couple, procedure, whether]        couple  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "q0IYiShkglx7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.6 Predict output word"
      ]
    },
    {
      "metadata": {
        "id": "zRFeh2WcglR3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get the probability distribution of the center word given context words"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iNNbXmHxg-OF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function that accepts lists of words as context and uses the model to determine\n",
        "# the most probable center word\n",
        "def findCenter(model, lists):\n",
        "  center = []\n",
        "  \n",
        "  for list in lists:\n",
        "    center.append(model.predict_output_word(list, topn=1))\n",
        "    \n",
        "  return(center)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jKVLlCr8hARL",
        "outputId": "d153d0e7-f895-4e3f-e0a3-a810701e8652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "cell_type": "code",
      "source": [
        "# List of 5 tokenized random phrases from corpus\n",
        "lists = [['pay','their','taxes','despite','low','likelihood'],\n",
        "        ['incorporate','potentially','given','loss','function'],\n",
        "        ['challenges','accurate','realistic','modeling'],\n",
        "        ['derive','macroscopic','statistics','different','types'],\n",
        "        ['critical','rapid','encoding','novel','information']]\n",
        "\n",
        "center_by_method = {}\n",
        "\n",
        "# Use the methods to determine the center word for each list of context words\n",
        "for (name,model) in models:\n",
        "  center_by_method[name] = findCenter(model, lists)\n",
        "\n",
        "print(pd.DataFrame.from_dict(center_by_method))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Basic Model      Min Token Frequency  \\\n",
            "0  [(model, 0.00025726782)]  [(model, 0.0011494253)]   \n",
            "1  [(model, 0.00025726782)]  [(model, 0.0011494253)]   \n",
            "2  [(model, 0.00025726782)]  [(model, 0.0011494253)]   \n",
            "3  [(model, 0.00025726782)]  [(model, 0.0011494253)]   \n",
            "4  [(model, 0.00025726782)]  [(model, 0.0011494253)]   \n",
            "\n",
            "          Negative Sampling           Train Algorithm  \\\n",
            "0  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "1  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "2  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "3  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "4  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "\n",
            "  Vector Dimensionality 250 Vector Dimensionality 500  \\\n",
            "0  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "1  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "2  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "3  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "4  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "\n",
            "                Window Size  \n",
            "0  [(model, 0.00025726782)]  \n",
            "1  [(model, 0.00025726782)]  \n",
            "2  [(model, 0.00025726782)]  \n",
            "3  [(model, 0.00025726782)]  \n",
            "4  [(model, 0.00025726782)]  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PL6ZVhTn_pJt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X64mmVtQNyRZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VhYcuiaJgD45",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIXWz4Y4NyCj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Bvv-0zLNx3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9QFUUJcUotIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DPHStF87k2e2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lGnA6xESlIrA"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1. Train a word embedding model on the sentence classification corpus from the UCI Machine Learning repository"
      ]
    },
    {
      "metadata": {
        "id": "DC6TITGKkxGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "5ba46460-c172-4fa7-bceb-a95b676ffe01",
        "id": "ixLV9Y7_M0Mg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "# Connect to personal Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p-gzqcMANitW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rune3cWSnMQs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download and extract data to Drive. Once done, you can handle the data locally."
      ]
    },
    {
      "metadata": {
        "id": "pNludKVRpSiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f8580c9-ca19-4b4e-936e-16ccb939cd0d"
      },
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00311/SentenceCorpus.zip\", filename=\"SentenceCorpus.zip\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('SentenceCorpus.zip', <http.client.HTTPMessage at 0x7f565c93e3c8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "HjIoE_lfNojr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00311/SentenceCorpus.zip\", filename=\"SentenceCorpus.zip\")\n",
        "with zipfile.ZipFile('SentenceCorpus.zip', 'r') as z:\n",
        "  z.extractall('/content/drive/My Drive/NLP Assignment 2/Part3/')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "q5_FYseGLYxq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data paths\n",
        "training_docs_directory = '/content/drive/My Drive/NLP Assignment 2/Part3/SentenceCorpus/labeled_articles/'\n",
        "official_stopwords = '/content/drive/My Drive/NLP Assignment 2/Part3/SentenceCorpus/word_lists/stopwords.txt'\n",
        "vocab_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/vocab.txt'\n",
        "embedding_word2vec_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/embedding_word2vec.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zs_nM3XzIiV9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Train an Embedding Layer on the DataSet\n"
      ]
    },
    {
      "metadata": {
        "id": "7AV_9ZLiIIoG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###3.2.1 Extract the DataSet's vocabulary and save it in a file to Drive for local use"
      ]
    },
    {
      "metadata": {
        "id": "f0OxSO3jtyTO",
        "colab_type": "code",
        "outputId": "524645b2-7ef4-4454-95c1-034ea19ad63e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "f23822cb-8f68-430c-d1ea-48e87ad8a50c",
        "id": "IlO_IMuvLYyU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "  doc = doc.lower()\n",
        "  tokens = doc.split()\n",
        "  # remove punctuation from each token\n",
        "  table = str.maketrans('', '', punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  tokens = [w for w in tokens if not w in extra_words_to_remove]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # update counts\n",
        "  vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory  \n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # keep docs only drom the first reviewer\n",
        "    if filename.endswith('1.txt'):\n",
        "      # create the full path of the file to open\n",
        "      path = directory+filename\n",
        "      # add doc to vocab\n",
        "      add_doc_to_vocab(path,vocab)\n",
        "\n",
        "# Save list to file\n",
        "def save_list(lines, filename):\n",
        "  # convert lines to a single blob of text\n",
        "  data = '\\n'.join(lines)\n",
        "  # open file\n",
        "  file = open(filename, 'w')\n",
        "  # write text\n",
        "  file.write(data)\n",
        "  # close file\n",
        "  file.close()\n",
        "\n",
        "# Add all docs to vocab\n",
        "# Define vocabulary\n",
        "vocab = Counter()\n",
        "\n",
        "extra_words_to_remove = ['citation','number','symbol','misc',\n",
        "                         'aimx','ownx','cont','base','of','',\n",
        "                         'abstract','introduction']\n",
        "\n",
        "# Decompress and manage DataSet\n",
        "# Get training documents from respective directories\n",
        "process_docs(training_docs_directory, vocab)\n",
        "\n",
        "# The size of the vocab\n",
        "print(\"\\nVocabulary size:\", len(vocab))\n",
        "# Top words in the vocab\n",
        "print(\"\\nMost common words: \\n\", vocab.most_common(50))\n",
        "\n",
        "# Keep tokens with a min occurence\n",
        "min_occurence = 1\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurence]\n",
        "print(\"\\nUpdated Vocabulary size:\", len(tokens))\n",
        "\n",
        "# Save tokens to a vocabulary file\n",
        "save_list(tokens, vocab_filename)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "Vocabulary size: 3968\n",
            "\n",
            "Most common words: \n",
            " [('model', 96), ('models', 58), ('one', 56), ('may', 50), ('many', 48), ('also', 48), ('based', 47), ('however', 46), ('proteins', 46), ('two', 45), ('data', 45), ('results', 43), ('learning', 41), ('loss', 38), ('selfcontrol', 38), ('participants', 38), ('used', 37), ('using', 36), ('behavior', 36), ('new', 35), ('different', 35), ('well', 35), ('stochastic', 34), ('thus', 34), ('section', 33), ('information', 32), ('studies', 32), ('ion', 32), ('general', 31), ('et', 31), ('al', 31), ('conflict', 31), ('formula', 31), ('neurons', 31), ('splicing', 31), ('expected', 30), ('use', 30), ('first', 30), ('study', 30), ('choice', 30), ('face', 30), ('example', 29), ('task', 29), ('network', 29), ('large', 28), ('analysis', 28), ('problem', 28), ('paper', 27), ('function', 27), ('individual', 27)]\n",
            "\n",
            "Updated Vocabulary size: 3968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G-G9i4ZrJi8C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 Training the Embedding Layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wcNXnLgGLovy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Optional. Recover vocabulary from local file\n",
        "with open(vocab_filename, \"r\") as f:\n",
        "    vocab = f.read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KpSfKHWBMDEF",
        "colab_type": "code",
        "outputId": "5d7b849f-b0f5-455e-c510-27900611cd52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3968"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "HL2qvmrY3C-t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2.1 Building the data"
      ]
    },
    {
      "metadata": {
        "id": "3PXmeO-6UM6x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Libraries and Functions to use**"
      ]
    },
    {
      "metadata": {
        "id": "p8hGhWCbtTTk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the data pre-processing I will follow different approach than the Part 2 of the Assignment, with various predefined functions and hopefully the same results."
      ]
    },
    {
      "metadata": {
        "id": "hlkRADPsUL65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76a016a3-f10b-4388-d0e4-ced26704594f"
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import operator\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "\n",
        "\n",
        "classes = ['aimx','base','cont','misc','ownx']\n",
        "extra_words_to_remove = ['citation','number','symbol','of','','abstract','introduction']\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def doc_to_clean_lines(doc, vocab):\n",
        "  clean_lines = []\n",
        "  lines = doc.splitlines()\n",
        "  for line in lines:\n",
        "    line = line.lower()\n",
        "    # split into tokens by white space\n",
        "    tokens = line.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    tokens = [w for w in tokens if not w in extra_words_to_remove]\n",
        "    # filter out tokens not in vocab\n",
        "    tokens = [w for w in tokens if w in vocab+classes]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    clean_lines.append(tokens)\n",
        "  return clean_lines\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, reviewer):\n",
        "  lines = []\n",
        "  # walk through all files in the folder\n",
        "  filelist = sorted(listdir(directory))\n",
        "  for filename in filelist:\n",
        "    # keep docs only from the first reviewer\n",
        "    if filename.endswith(reviewer):\n",
        "      # create the full path of the file to open\n",
        "      path = directory + filename\n",
        "      # load and clean the doc\n",
        "      doc = load_doc(path)\n",
        "      doc_lines = doc_to_clean_lines(doc, vocab)\n",
        "      doc_lines = [l for l in doc_lines if len(l) > 0]\n",
        "      doc_lines = [l for l in doc_lines if l[0] in classes]\n",
        "      # add lines to list\n",
        "      lines += doc_lines\n",
        "  return lines\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "yE2oAw7nUuN_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Get the training data**"
      ]
    },
    {
      "metadata": {
        "id": "Vr1bapqMU1Tt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Corpus doc (X) and labels (y) from DataSet\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Get training documents from respective directories\n",
        "# Each element is a single line (list of clean tokens) of the training documents\n",
        "# Getting corpus docs (X) and labels from first reviewer's files\n",
        "training_docs = process_docs(training_docs_directory, vocab, '1.txt')\n",
        "y1 = []   # Labels from first reviewer\n",
        "for line in training_docs:\n",
        "  y1.append(line[0])\n",
        "  X.append(line[1:])\n",
        "\n",
        "# Labels from second and third reviewers\n",
        "y2 = []\n",
        "training_docs2 = process_docs(training_docs_directory, vocab, '2.txt')\n",
        "for line in training_docs2:\n",
        "  y2.append(line[0])\n",
        "y3 = []\n",
        "training_docs3 = process_docs(training_docs_directory, vocab, '3.txt')\n",
        "for line in training_docs3:\n",
        "  y3.append(line[0])\n",
        "\n",
        "# get the predominant label for each doc\n",
        "for i in range(len(y1)):\n",
        "    dic = {'aimx': 0, 'base': 0, 'cont': 0, 'misc': 0, 'ownx': 0, }\n",
        "    dic[y1[i]] += 1\n",
        "    dic[y2[i]] += 1\n",
        "    dic[y3[i]] += 1\n",
        "    sorted_dic = sorted(dic.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    y.append(sorted_dic[0][0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mAU3BkTTwug1",
        "colab_type": "code",
        "outputId": "18e8ccc3-0012-4d95-afba-49de2093d4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "y[:10]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['misc',\n",
              " 'misc',\n",
              " 'misc',\n",
              " 'aimx',\n",
              " 'ownx',\n",
              " 'ownx',\n",
              " 'ownx',\n",
              " 'misc',\n",
              " 'cont',\n",
              " 'misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "_RXvSfJ8VEDx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Handling categorical y values**"
      ]
    },
    {
      "metadata": {
        "id": "CyOp47tSVDt9",
        "colab_type": "code",
        "outputId": "78f244b3-cb86-4edd-9997-25fdd53fbf58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "# First transform categorical values to integers\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "print('Original y labels: ', classes)\n",
        "print('Integer encoded y labels: ', le.transform(classes))\n",
        "\n",
        "# Then use One Hot encoding\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "y = y.reshape(len(y),1)\n",
        "y = ohe.fit_transform(y)\n",
        "# One could use only the initial Integer encoder and not the following One Hot.\n",
        "# Then during Network compilation should use loss='sparse_categorical_crossentropy'\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original y labels:  ['aimx', 'base', 'cont', 'misc', 'ownx']\n",
            "Integer encoded y labels:  [0 1 2 3 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "enqN8MxJIgEL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split training-testing data\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,\n",
        "                                               stratify=y, random_state=0)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# encode sequences\n",
        "encoded_X_train = tokenizer.texts_to_sequences(X_train)\n",
        "encoded_X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s) for s in X])\n",
        "X_train_pad = pad_sequences(encoded_X_train, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(encoded_X_test, maxlen=max_length, padding='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pbOvNvr3Q83",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2.2 Define and Train the model\n",
        "\n",
        "I define the model inside a function in order to be able to call it on the next part of the Assignment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "5033054d-40be-48e2-be63-015ac7c132d1",
        "id": "wHpjPLmM6pAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Model's parameters\n",
        "output_dim = 100\n",
        "activation = 'softmax'\n",
        "losses = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "\n",
        "parameters = [output_dim, activation, losses, optimizer]\n",
        "\n",
        "def annDef(param):\n",
        "  # Define Model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, output_dim=output_dim, input_length=max_length))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(5, activation=activation))\n",
        "  print(model.summary())\n",
        "\n",
        "  # Compile Network\n",
        "  model.compile(loss=losses, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  # Fit Network to training data\n",
        "  model.fit(X_train_pad, y_train, epochs=10, verbose=2)\n",
        "\n",
        "  # Evaluate Network during Training\n",
        "  loss, acc = model.evaluate(X_test_pad, y_test, verbose=2)\n",
        "  print('Test Accuracy: %f' % (acc*100))\n",
        "  \n",
        "  return model\n",
        "  \n",
        "\n",
        "model = annDef(parameters)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/9\n",
            " - 0s - loss: 1.1738 - acc: 0.5683\n",
            "Epoch 2/9\n",
            " - 0s - loss: 0.9924 - acc: 0.6024\n",
            "Epoch 3/9\n",
            " - 0s - loss: 0.8682 - acc: 0.6171\n",
            "Epoch 4/9\n",
            " - 0s - loss: 0.6933 - acc: 0.8085\n",
            "Epoch 5/9\n",
            " - 0s - loss: 0.4873 - acc: 0.8902\n",
            "Epoch 6/9\n",
            " - 0s - loss: 0.3218 - acc: 0.9293\n",
            "Epoch 7/9\n",
            " - 0s - loss: 0.2072 - acc: 0.9646\n",
            "Epoch 8/9\n",
            " - 0s - loss: 0.1354 - acc: 0.9915\n",
            "Epoch 9/9\n",
            " - 0s - loss: 0.0885 - acc: 0.9939\n",
            "Test Accuracy: 66.829268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BTQIKu_BqxFp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2.3 Model evaluation"
      ]
    },
    {
      "metadata": {
        "id": "oIC1NpqPrQdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing the metrics**\n",
        "\n",
        "I will use the well known metrics from sklearn library"
      ]
    },
    {
      "metadata": {
        "id": "QtkACVCRkw5z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "goMFQhr9rkfl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Prediction on the test set**\n",
        "\n",
        "Use trained model to extract the predictions for the test set of data"
      ]
    },
    {
      "metadata": {
        "id": "OBWKxT9lkwr2",
        "colab_type": "code",
        "outputId": "ff2fc9d5-aaea-4df4-a039-84be538a0955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_pad)\n",
        "y_pred[:10]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01635011, 0.00768984, 0.01620445, 0.9035947 , 0.05616083],\n",
              "       [0.06417371, 0.01832901, 0.04484073, 0.3990794 , 0.4735771 ],\n",
              "       [0.07827186, 0.03355028, 0.05857874, 0.49991858, 0.3296805 ],\n",
              "       [0.06035602, 0.04286615, 0.13120122, 0.6543308 , 0.11124577],\n",
              "       [0.01870227, 0.00959134, 0.03747429, 0.9101905 , 0.02404161],\n",
              "       [0.05441982, 0.01605988, 0.02669431, 0.5027342 , 0.4000917 ],\n",
              "       [0.10939447, 0.03267734, 0.07525317, 0.4094746 , 0.37320042],\n",
              "       [0.05899882, 0.07121229, 0.28040886, 0.5207333 , 0.06864672],\n",
              "       [0.01490743, 0.00607549, 0.01438066, 0.9210328 , 0.04360374],\n",
              "       [0.02601239, 0.00933846, 0.02204727, 0.850629  , 0.09197292]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "uX3xNC9VsPhK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's obvious that the prediction for each sample is in the SoftMax format. The output is composed by 5 float numbers denoting the probability the sample to belong to each one of the 5 categories."
      ]
    },
    {
      "metadata": {
        "id": "q2rSPdQotqap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Turning probabilities into Labels**\n",
        "\n",
        "My task is to restore the original labels for testing data. I will follow the opposite procedure than the one used to encode the categorical y values:\n",
        "\n",
        "- Turn SoftMax format into Integer encoding\n",
        "- Turn Integer encoding into original labels, using the original fitted LabelEncoder() instance (inverse transformation)."
      ]
    },
    {
      "metadata": {
        "id": "OTFJJ78dk1n9",
        "colab_type": "code",
        "outputId": "9b38ac3b-79ac-4e3e-cd48-e36687cd912a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Integer encoding\n",
        "# argmax() function will replace the set of probabilities with the index of\n",
        "# the higher probability \n",
        "y_p = np.argmax(y_pred, axis=1)\n",
        "y_p_t = np.argmax(y_test, axis=1)\n",
        "y_p[:10]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4, 3, 3, 3, 3, 3, 3, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "2Ya6dUiXlbzN",
        "colab_type": "code",
        "outputId": "a316822c-2818-461c-84de-54af9b7c33dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# Inverse transformation of Integer encoding to Labels\n",
        "y_predictions = le.inverse_transform(y_p)\n",
        "y_true = le.inverse_transform(y_p_t)\n",
        "y_predictions[:10]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['misc', 'ownx', 'misc', 'misc', 'misc', 'misc', 'misc', 'misc',\n",
              "       'misc', 'misc'], dtype='<U4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "OAn9rJri3Nl_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Calculate the metrics**"
      ]
    },
    {
      "metadata": {
        "id": "nY-mC48wlbGF",
        "colab_type": "code",
        "outputId": "cf946881-edec-4fad-8882-71e43bd090cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_true,y_predictions)\n",
        "recall = recall_score(y_true,y_predictions,average='macro')\n",
        "precision = precision_score(y_true,y_predictions,average='macro')\n",
        "f1 = f1_score(y_true,y_predictions,average='macro')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "2H-YYCw4la2j",
        "colab_type": "code",
        "outputId": "1fb68f94-70f9-4970-92d3-84a3d6b5405d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "print('Accuracy:\\t{:.3f}'.format(accuracy))\n",
        "print('Recall:\\t\\t{:.3f}'.format(recall))\n",
        "print('Precision:\\t{:.3f}'.format(precision))\n",
        "print('F1 score:\\t{:.3f}'.format(f1))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:\t0.668\n",
            "Recall:\t\t0.276\n",
            "Precision:\t0.460\n",
            "F1 score:\t0.276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a5h9RrolNxd4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZA4AGJklKz0I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Experimenting with various parameters of the Classifier"
      ]
    },
    {
      "metadata": {
        "id": "XkeJpIDYOxNJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model definition function from previous part**"
      ]
    },
    {
      "metadata": {
        "id": "HhCBZUKeOwgH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def annDef(od, act, los, opt):\n",
        "  # Define Model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, output_dim=od, input_length=max_length))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(5, activation=act))\n",
        "  print(model.summary())\n",
        "\n",
        "  # Compile Network\n",
        "  model.compile(loss=los, optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "  # Fit Network to training data\n",
        "  model.fit(X_train_pad, y_train, epochs=10, verbose=2)\n",
        "\n",
        "  # Evaluate Network during Training\n",
        "  loss, acc = model.evaluate(X_test_pad, y_test, verbose=2)\n",
        "  print('Test Accuracy: %f' % (acc*100))\n",
        "  \n",
        "  return(model)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cwTpybZ1R2SR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Model evaluation function**"
      ]
    },
    {
      "metadata": {
        "id": "XPojGkU0R17N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evalModel(model):\n",
        "  y_pred = model.predict(X_test_pad)\n",
        "  y_p = np.argmax(y_pred, axis=1)\n",
        "  y_p_t = np.argmax(y_test, axis=1)\n",
        "  y_predictions = le.inverse_transform(y_p)\n",
        "  y_true = le.inverse_transform(y_p_t)\n",
        "  accuracy = accuracy_score(y_true,y_predictions)\n",
        "  recall = recall_score(y_true,y_predictions,average='macro')\n",
        "  precision = precision_score(y_true,y_predictions,average='macro')\n",
        "  f1 = f1_score(y_true,y_predictions,average='macro')\n",
        "  return((accuracy,recall,precision,f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ZusO0fZPCi6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Define model's parameters and execute 3 times with the same set**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "9d8f247b-1968-4b8e-9955-85ebdc840b23",
        "id": "MbsGkVP3LUYw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 43870
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
        "\n",
        "# Dataframe to store total results\n",
        "headers=['Dimensions','Activation','Losses','Optimizer','Accurasy(+/-sd)',\n",
        "         'Recall(+/-sd)','Precision(+/-sd)','F1_score(+/-sd)']\n",
        "headers2 = ['Accurasy','Recall','Precision','F1_score']\n",
        "\n",
        "total = pd.DataFrame(columns=headers)\n",
        "\n",
        "# Model's parameters' range\n",
        "output_dim = [100,250,500]\n",
        "activation = ['softmax','sigmoid']\n",
        "losses = ['categorical_crossentropy','binary_crossentropy']\n",
        "optimizer = ['adam','sgd']\n",
        "\n",
        "# Testing set\n",
        "# output_dim = [100]\n",
        "# activation = ['softmax',]\n",
        "# losses = ['categorical_crossentropy',]\n",
        "# optimizer = ['adam',]\n",
        "\n",
        "for od in output_dim:\n",
        "  for act in activation:\n",
        "    for los in losses:\n",
        "      for opt in optimizer:\n",
        "        \n",
        "        parameters = [od, act, los, opt]\n",
        "        # List to store the results of each set\n",
        "        results = []\n",
        "        # Dataframe to store the results of each run\n",
        "        temp = pd.DataFrame(columns=headers2)\n",
        "        \n",
        "        # Execute model 3 times for each set\n",
        "        for i in range(3):\n",
        "          model = annDef(od, act, los, opt)\n",
        "          (accuracy,recall,precision,f1) = evalModel(model)\n",
        "          # Store run results as a new row to dataframe\n",
        "          new_row = pd.Series([accuracy,recall,precision,f1], index=headers2)\n",
        "          temp = temp.append(new_row, ignore_index=1)\n",
        "        \n",
        "        # Get the statistics of the 3 executions\n",
        "        acc = temp.Accurasy.mean()\n",
        "        accSd = temp.Accurasy.std()\n",
        "        rec = temp.Recall.mean()\n",
        "        recSd = temp.Recall.std()\n",
        "        pre = temp.Precision.mean()\n",
        "        preSd = temp.Precision.std()\n",
        "        f1 = temp.F1_score.mean()\n",
        "        f1Sd = temp.F1_score.std()\n",
        "        \n",
        "        # Statistics as stings\n",
        "        accStr = '%.3f+/-%.3f' % (acc, accSd)\n",
        "        recStr = '%.3f+/-%.3f' % (rec, recSd)\n",
        "        preStr = '%.3f+/-%.3f' % (pre, preSd)\n",
        "        f1Str = '%.3f+/-%.3f' % (f1, f1Sd)\n",
        "        \n",
        "        stats = [accStr,recStr,preStr,f1Str]\n",
        "        \n",
        "        # Store the final results of set\n",
        "        results.extend(parameters)\n",
        "        results.extend(stats)\n",
        "        \n",
        "        new_row = pd.Series(results, index=headers)\n",
        "        total = total.append(new_row, ignore_index=1)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 0s - loss: 1.1467 - acc: 0.5768\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9801 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8639 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.6917 - acc: 0.8134\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4825 - acc: 0.8866\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3150 - acc: 0.9439\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2011 - acc: 0.9732\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1282 - acc: 0.9915\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0859 - acc: 0.9963\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0596 - acc: 0.9988\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 0s - loss: 1.1405 - acc: 0.5646\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9861 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8745 - acc: 0.6463\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.6995 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4937 - acc: 0.8890\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3228 - acc: 0.9280\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2068 - acc: 0.9646\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1328 - acc: 0.9902\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0888 - acc: 0.9976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0606 - acc: 0.9988\n",
            "Test Accuracy: 67.804878\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 0s - loss: 1.1563 - acc: 0.5585\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9815 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8696 - acc: 0.6451\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7000 - acc: 0.7878\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4945 - acc: 0.8866\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3243 - acc: 0.9256\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2066 - acc: 0.9695\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1332 - acc: 0.9902\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0894 - acc: 0.9976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0610 - acc: 0.9976\n",
            "Test Accuracy: 69.268293\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 0s - loss: 1.4140 - acc: 0.6000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.1723 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0833 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0590 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0498 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0450 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0411 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0411 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0389 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0370 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 0s - loss: 1.4648 - acc: 0.5720\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.2126 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0979 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0640 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0517 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0458 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0433 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0411 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0397 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0378 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 0s - loss: 1.4565 - acc: 0.5585\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.2034 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0944 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0627 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0512 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0466 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0435 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0409 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0390 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0383 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.3820 - acc: 0.8251\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3306 - acc: 0.8439\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2925 - acc: 0.8680\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2308 - acc: 0.9415\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1636 - acc: 0.9693\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1149 - acc: 0.9744\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0780 - acc: 0.9820\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0527 - acc: 0.9898\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0352 - acc: 0.9954\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0237 - acc: 0.9983\n",
            "Test Accuracy: 87.317072\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.3823 - acc: 0.8241\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3323 - acc: 0.8461\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2922 - acc: 0.8673\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2299 - acc: 0.9351\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1628 - acc: 0.9698\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1121 - acc: 0.9744\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0783 - acc: 0.9849\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0511 - acc: 0.9890\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0349 - acc: 0.9966\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0231 - acc: 0.9983\n",
            "Test Accuracy: 87.121951\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.3770 - acc: 0.8283\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3311 - acc: 0.8432\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2927 - acc: 0.8766\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2301 - acc: 0.9239\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1630 - acc: 0.9683\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1116 - acc: 0.9756\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0757 - acc: 0.9839\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0506 - acc: 0.9910\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0339 - acc: 0.9956\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0228 - acc: 0.9983\n",
            "Test Accuracy: 87.609755\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.4919 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4713 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4508 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4304 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4111 - acc: 0.8000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3947 - acc: 0.8000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3826 - acc: 0.8007\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3743 - acc: 0.8276\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3690 - acc: 0.8354\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3654 - acc: 0.8378\n",
            "Test Accuracy: 83.902436\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.4931 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4715 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4506 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4302 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4108 - acc: 0.8000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3949 - acc: 0.8000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3832 - acc: 0.8063\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3753 - acc: 0.8280\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3701 - acc: 0.8363\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3665 - acc: 0.8376\n",
            "Test Accuracy: 83.999998\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.4942 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4764 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4585 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4398 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4211 - acc: 0.8000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4042 - acc: 0.8000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3902 - acc: 0.8000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3800 - acc: 0.8127\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3731 - acc: 0.8300\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3684 - acc: 0.8376\n",
            "Test Accuracy: 83.902436\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.2344 - acc: 0.5305\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0170 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9595 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8773 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7482 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.5253 - acc: 0.7976\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2240 - acc: 0.9500\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1054 - acc: 0.9878\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0615 - acc: 0.9951\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0414 - acc: 0.9976\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.2231 - acc: 0.5598\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0141 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9573 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8806 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7560 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.5327 - acc: 0.8183\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2264 - acc: 0.9524\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1080 - acc: 0.9902\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0627 - acc: 0.9951\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0403 - acc: 0.9976\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.2145 - acc: 0.5805\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0166 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9571 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8709 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7419 - acc: 0.6024\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4671 - acc: 0.8512\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1983 - acc: 0.9683\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0985 - acc: 0.9927\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0595 - acc: 0.9963\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0392 - acc: 0.9976\n",
            "Test Accuracy: 67.804878\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_18 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_18 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.5656 - acc: 0.5634\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.4822 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.3920 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.2957 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.2055 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.1360 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0930 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0691 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0559 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0484 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_19 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.5746 - acc: 0.5427\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.4914 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.4029 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.3085 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.2173 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.1455 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.1004 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0739 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0594 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0516 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_20 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_20 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.5759 - acc: 0.5390\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.4989 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.4146 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.3211 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.2295 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.1546 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.1043 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0762 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0615 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0533 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_21 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.4522 - acc: 0.8334\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3451 - acc: 0.8400\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3217 - acc: 0.8405\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2851 - acc: 0.8917\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2303 - acc: 0.9268\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1755 - acc: 0.9698\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1288 - acc: 0.9754\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0941 - acc: 0.9824\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0693 - acc: 0.9868\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0520 - acc: 0.9920\n",
            "Test Accuracy: 87.024389\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_22 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_22 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.4482 - acc: 0.8241\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3443 - acc: 0.8400\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3208 - acc: 0.8439\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2839 - acc: 0.8920\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2296 - acc: 0.9427\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1743 - acc: 0.9695\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1270 - acc: 0.9763\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0937 - acc: 0.9812\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0697 - acc: 0.9868\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0520 - acc: 0.9917\n",
            "Test Accuracy: 86.536581\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_23 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_23 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.4512 - acc: 0.8188\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3457 - acc: 0.8424\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3217 - acc: 0.8490\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2846 - acc: 0.8729\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2312 - acc: 0.9510\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1775 - acc: 0.9610\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1290 - acc: 0.9744\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0951 - acc: 0.9815\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0701 - acc: 0.9873\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0524 - acc: 0.9912\n",
            "Test Accuracy: 86.829265\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_24 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_24 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.6806 - acc: 0.7173\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.6458 - acc: 0.8407\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.6088 - acc: 0.8402\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5681 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5254 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4846 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4496 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4223 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.4027 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3887 - acc: 0.8400\n",
            "Test Accuracy: 83.902436\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_25 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_25 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.6747 - acc: 0.7083\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.6355 - acc: 0.8139\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5943 - acc: 0.8388\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5505 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5066 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4671 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4348 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4106 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3935 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3818 - acc: 0.8400\n",
            "Test Accuracy: 83.902436\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_26 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_26 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.6754 - acc: 0.7520\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.6379 - acc: 0.8385\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5981 - acc: 0.8400\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5551 - acc: 0.8402\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5116 - acc: 0.8402\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4716 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4388 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4141 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3964 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3842 - acc: 0.8400\n",
            "Test Accuracy: 83.999997\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_27 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_27 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.1138 - acc: 0.5854\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9151 - acc: 0.6024\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7011 - acc: 0.8220\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4351 - acc: 0.8902\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2296 - acc: 0.9634\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1212 - acc: 0.9878\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0661 - acc: 0.9976\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0412 - acc: 0.9988\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0279 - acc: 0.9988\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0205 - acc: 0.9988\n",
            "Test Accuracy: 67.804878\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_28 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_28 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.1135 - acc: 0.5756\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9302 - acc: 0.6012\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7063 - acc: 0.8085\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4256 - acc: 0.9085\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2270 - acc: 0.9634\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1195 - acc: 0.9939\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0651 - acc: 0.9976\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0410 - acc: 0.9988\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0277 - acc: 0.9988\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0204 - acc: 0.9988\n",
            "Test Accuracy: 68.292683\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_29 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_29 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.1160 - acc: 0.5988\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9151 - acc: 0.6037\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7019 - acc: 0.7646\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4333 - acc: 0.9012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2259 - acc: 0.9622\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1187 - acc: 0.9939\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0664 - acc: 0.9976\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0411 - acc: 0.9988\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0277 - acc: 0.9988\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0202 - acc: 0.9988\n",
            "Test Accuracy: 68.780488\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_30 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_30 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.3698 - acc: 0.5780\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.1270 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0687 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0520 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0453 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0424 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0388 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0371 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0358 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0344 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_31 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_31 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.3863 - acc: 0.5915\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.1371 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0701 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0524 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0450 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0414 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0385 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0367 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0352 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0334 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_32 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_32 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.3944 - acc: 0.5573\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.1366 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0708 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0538 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0475 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0438 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0411 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0387 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0362 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0349 - acc: 0.6000\n",
            "Test Accuracy: 59.512195\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_33 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_33 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.3688 - acc: 0.8363\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3103 - acc: 0.8507\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2364 - acc: 0.9232\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1453 - acc: 0.9702\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0832 - acc: 0.9829\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0459 - acc: 0.9924\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0254 - acc: 0.9973\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0147 - acc: 0.9995\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0096 - acc: 0.9998\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0068 - acc: 0.9998\n",
            "Test Accuracy: 86.829265\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_34 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_34 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.3692 - acc: 0.8332\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3105 - acc: 0.8593\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2412 - acc: 0.9234\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1501 - acc: 0.9693\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0843 - acc: 0.9810\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0468 - acc: 0.9927\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0262 - acc: 0.9971\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0151 - acc: 0.9990\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0096 - acc: 0.9998\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0070 - acc: 0.9998\n",
            "Test Accuracy: 86.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_35 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.3743 - acc: 0.8322\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3119 - acc: 0.8602\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2375 - acc: 0.9178\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1455 - acc: 0.9710\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0824 - acc: 0.9832\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0455 - acc: 0.9922\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0248 - acc: 0.9978\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0146 - acc: 0.9993\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0095 - acc: 0.9995\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0067 - acc: 0.9998\n",
            "Test Accuracy: 87.707315\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_36 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_36 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4785 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4452 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4176 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3965 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3820 - acc: 0.8068\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3731 - acc: 0.8341\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3675 - acc: 0.8368\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3640 - acc: 0.8383\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3614 - acc: 0.8395\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3593 - acc: 0.8400\n",
            "Test Accuracy: 83.902438\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_37 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_37 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4781 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4422 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4135 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3931 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3798 - acc: 0.8127\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3715 - acc: 0.8320\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3665 - acc: 0.8378\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3630 - acc: 0.8393\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3606 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3587 - acc: 0.8402\n",
            "Test Accuracy: 84.195119\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_38 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_38 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4863 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4523 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4233 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4005 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3846 - acc: 0.8032\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3745 - acc: 0.8224\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3682 - acc: 0.8366\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3643 - acc: 0.8388\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3615 - acc: 0.8388\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3594 - acc: 0.8395\n",
            "Test Accuracy: 83.902438\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_39 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_39 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.1392 - acc: 0.5610\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9807 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8809 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7142 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4505 - acc: 0.8183\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1182 - acc: 0.9878\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0457 - acc: 0.9951\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0249 - acc: 0.9988\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0169 - acc: 0.9988\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0121 - acc: 1.0000\n",
            "Test Accuracy: 67.804878\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_40 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_40 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.1540 - acc: 0.5720\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9814 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8788 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7170 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4765 - acc: 0.7622\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1341 - acc: 0.9854\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0463 - acc: 0.9951\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0264 - acc: 0.9976\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0172 - acc: 0.9988\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0120 - acc: 1.0000\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_41 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_41 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.1377 - acc: 0.5732\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9863 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8771 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7203 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4086 - acc: 0.8744\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1113 - acc: 0.9890\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0446 - acc: 0.9963\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0245 - acc: 0.9988\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0166 - acc: 0.9988\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0119 - acc: 1.0000\n",
            "Test Accuracy: 67.317073\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_42 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_42 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.5378 - acc: 0.5976\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.4171 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.3067 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.2080 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.1342 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0898 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0663 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0537 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0472 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0437 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_43 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_43 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.5462 - acc: 0.5329\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.4213 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.3087 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.2092 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.1361 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0925 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0686 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0560 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0487 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0448 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_44 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_44 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.5367 - acc: 0.5939\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.4172 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.3071 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.2101 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.1385 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0936 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0696 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0571 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0501 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0457 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_45 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_45 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4152 - acc: 0.8132\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3365 - acc: 0.8441\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2865 - acc: 0.8744\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2136 - acc: 0.9505\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1387 - acc: 0.9744\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0870 - acc: 0.9834\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0552 - acc: 0.9915\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0361 - acc: 0.9961\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0244 - acc: 0.9985\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0176 - acc: 0.9995\n",
            "Test Accuracy: 86.731705\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_46 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_46 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4145 - acc: 0.8212\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3344 - acc: 0.8559\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2847 - acc: 0.8668\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2122 - acc: 0.9454\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1378 - acc: 0.9751\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0856 - acc: 0.9832\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0549 - acc: 0.9915\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0357 - acc: 0.9956\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0241 - acc: 0.9985\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0172 - acc: 0.9995\n",
            "Test Accuracy: 86.829267\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_47 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_47 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4082 - acc: 0.8278\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3338 - acc: 0.8468\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2843 - acc: 0.8800\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2117 - acc: 0.9546\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1384 - acc: 0.9737\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0876 - acc: 0.9841\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0562 - acc: 0.9900\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0364 - acc: 0.9959\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0247 - acc: 0.9990\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0174 - acc: 0.9995\n",
            "Test Accuracy: 86.536583\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_48 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_48 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.6584 - acc: 0.7832\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5946 - acc: 0.8393\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5374 - acc: 0.8405\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4878 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4482 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4192 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3990 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3855 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3765 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3702 - acc: 0.8400\n",
            "Test Accuracy: 83.999997\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_49 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_49 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.6610 - acc: 0.7537\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5972 - acc: 0.8390\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5400 - acc: 0.8400\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4901 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4501 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4205 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3999 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3862 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3768 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3704 - acc: 0.8400\n",
            "Test Accuracy: 83.999997\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_50 (Embedding)     (None, 54, 250)           980250    \n",
            "_________________________________________________________________\n",
            "flatten_50 (Flatten)         (None, 13500)             0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 5)                 67505     \n",
            "=================================================================\n",
            "Total params: 1,047,755\n",
            "Trainable params: 1,047,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.6722 - acc: 0.6968\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.6129 - acc: 0.8402\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5580 - acc: 0.8400\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5075 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4646 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4315 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4078 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3913 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3803 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3728 - acc: 0.8400\n",
            "Test Accuracy: 83.902436\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_51 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_51 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.1523 - acc: 0.5707\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.8390 - acc: 0.6744\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.5069 - acc: 0.8841\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.2269 - acc: 0.9671\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0945 - acc: 0.9939\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0469 - acc: 0.9988\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0275 - acc: 0.9988\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0185 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0132 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0100 - acc: 1.0000\n",
            "Test Accuracy: 68.292683\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_52 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_52 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.0880 - acc: 0.5878\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.8303 - acc: 0.6732\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.5051 - acc: 0.8951\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.2230 - acc: 0.9622\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0940 - acc: 0.9927\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0466 - acc: 0.9988\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0278 - acc: 0.9988\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0185 - acc: 0.9988\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0132 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0099 - acc: 1.0000\n",
            "Test Accuracy: 67.804878\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_53 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_53 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.1054 - acc: 0.5805\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.8308 - acc: 0.6537\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.5013 - acc: 0.8902\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.2238 - acc: 0.9622\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0936 - acc: 0.9951\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0463 - acc: 0.9988\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0272 - acc: 0.9988\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0180 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0131 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0099 - acc: 1.0000\n",
            "Test Accuracy: 67.317073\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_54 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_54 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.2946 - acc: 0.5817\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0867 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0563 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0460 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0417 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0379 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0349 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0331 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0298 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0276 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_55 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_55 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.2962 - acc: 0.5756\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0860 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0547 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0445 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0395 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0352 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0340 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0308 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0287 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0258 - acc: 0.6000\n",
            "Test Accuracy: 59.512195\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_56 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_56 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.3072 - acc: 0.5744\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0881 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0545 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0446 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0397 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0348 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0317 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0299 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0274 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0249 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_57 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_57 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.3735 - acc: 0.8329\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.2847 - acc: 0.8780\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.1766 - acc: 0.9666\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.0841 - acc: 0.9820\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0380 - acc: 0.9937\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0173 - acc: 0.9990\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0097 - acc: 0.9995\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0061 - acc: 0.9998\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0043 - acc: 0.9998\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0032 - acc: 1.0000\n",
            "Test Accuracy: 87.707315\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_58 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_58 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.3741 - acc: 0.8346\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.2879 - acc: 0.8883\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.1754 - acc: 0.9534\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.0843 - acc: 0.9837\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0384 - acc: 0.9946\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0173 - acc: 0.9990\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0097 - acc: 0.9995\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0061 - acc: 0.9998\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0044 - acc: 0.9998\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0032 - acc: 1.0000\n",
            "Test Accuracy: 87.707317\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_59 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_59 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.3868 - acc: 0.8244\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.2875 - acc: 0.8810\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.1831 - acc: 0.9656\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.0876 - acc: 0.9822\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0390 - acc: 0.9937\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0186 - acc: 0.9993\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0099 - acc: 0.9995\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0063 - acc: 0.9998\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0045 - acc: 0.9998\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0033 - acc: 1.0000\n",
            "Test Accuracy: 86.926829\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_60 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_60 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.4711 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4232 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3935 - acc: 0.7988\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3777 - acc: 0.8205\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3693 - acc: 0.8349\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3643 - acc: 0.8388\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3610 - acc: 0.8393\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3586 - acc: 0.8395\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3569 - acc: 0.8398\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3555 - acc: 0.8395\n",
            "Test Accuracy: 83.999998\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_61 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_61 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.4728 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4240 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3937 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3773 - acc: 0.8202\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3687 - acc: 0.8376\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3638 - acc: 0.8385\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3604 - acc: 0.8395\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3579 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3563 - acc: 0.8402\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3549 - acc: 0.8400\n",
            "Test Accuracy: 83.902437\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_62 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_62 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.4756 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4235 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3923 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3765 - acc: 0.8224\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3684 - acc: 0.8376\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3637 - acc: 0.8383\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3605 - acc: 0.8395\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3582 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3566 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3553 - acc: 0.8402\n",
            "Test Accuracy: 83.999999\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_63 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_63 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.1180 - acc: 0.5817\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.9514 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.7772 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.5591 - acc: 0.6122\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.2026 - acc: 0.9634\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0345 - acc: 0.9951\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0149 - acc: 0.9988\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0093 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0069 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0050 - acc: 1.0000\n",
            "Test Accuracy: 68.780488\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_64 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_64 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.1267 - acc: 0.5793\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.9513 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.7737 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.5541 - acc: 0.6207\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.1834 - acc: 0.9720\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0345 - acc: 0.9951\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0157 - acc: 0.9988\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0093 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0065 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0052 - acc: 1.0000\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_65 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_65 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.1314 - acc: 0.5793\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.9576 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.7782 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.5439 - acc: 0.6671\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.1395 - acc: 0.9817\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0313 - acc: 0.9976\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0152 - acc: 0.9988\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0100 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0071 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0053 - acc: 1.0000\n",
            "Test Accuracy: 66.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_66 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_66 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.5036 - acc: 0.5720\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.3376 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.2156 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.1337 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0872 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0638 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0523 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0464 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0429 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0405 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_67 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_67 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.4996 - acc: 0.5866\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.3309 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.2086 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.1277 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0837 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0612 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0500 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0441 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0406 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0385 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_68 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_68 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.5016 - acc: 0.5695\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.3424 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.2229 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.1400 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0920 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0669 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0543 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0470 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0432 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0411 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_69 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_69 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.3924 - acc: 0.8315\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.3171 - acc: 0.8566\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.2357 - acc: 0.9302\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.1392 - acc: 0.9741\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0754 - acc: 0.9868\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0413 - acc: 0.9944\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0241 - acc: 0.9983\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0156 - acc: 0.9995\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0109 - acc: 0.9998\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0081 - acc: 0.9998\n",
            "Test Accuracy: 87.219510\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_70 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_70 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.3926 - acc: 0.8280\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.3136 - acc: 0.8524\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.2354 - acc: 0.9280\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.1388 - acc: 0.9751\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0744 - acc: 0.9873\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0407 - acc: 0.9949\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0240 - acc: 0.9983\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0158 - acc: 0.9995\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0109 - acc: 0.9998\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0081 - acc: 0.9998\n",
            "Test Accuracy: 87.024388\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_71 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_71 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.3999 - acc: 0.8183\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.3175 - acc: 0.8678\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.2398 - acc: 0.9156\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.1434 - acc: 0.9695\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0771 - acc: 0.9871\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0416 - acc: 0.9920\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0249 - acc: 0.9980\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0162 - acc: 0.9995\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0113 - acc: 0.9998\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0083 - acc: 0.9998\n",
            "Test Accuracy: 86.243901\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_72 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_72 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.6389 - acc: 0.7961\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5489 - acc: 0.8402\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4848 - acc: 0.8402\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4404 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4111 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3921 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3800 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3720 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3667 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3631 - acc: 0.8400\n",
            "Test Accuracy: 83.999997\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_73 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_73 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.6456 - acc: 0.7863\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5572 - acc: 0.8398\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4923 - acc: 0.8400\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4463 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4152 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3949 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3819 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3733 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3677 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3636 - acc: 0.8400\n",
            "Test Accuracy: 83.999997\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_74 (Embedding)     (None, 54, 500)           1960500   \n",
            "_________________________________________________________________\n",
            "flatten_74 (Flatten)         (None, 27000)             0         \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 5)                 135005    \n",
            "=================================================================\n",
            "Total params: 2,095,505\n",
            "Trainable params: 2,095,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.6439 - acc: 0.7788\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5549 - acc: 0.8410\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4901 - acc: 0.8400\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4443 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4138 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3940 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3813 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3730 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3674 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3635 - acc: 0.8400\n",
            "Test Accuracy: 83.999997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n_hwG_4nqWYg",
        "colab_type": "code",
        "outputId": "b3dfe385-7074-47de-a81f-0b3a3e22437f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "cell_type": "code",
      "source": [
        "total"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dimensions</th>\n",
              "      <th>Activation</th>\n",
              "      <th>Losses</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>Accurasy(+/-sd)</th>\n",
              "      <th>Recall(+/-sd)</th>\n",
              "      <th>Precision(+/-sd)</th>\n",
              "      <th>F1_score(+/-sd)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.676+/-0.017</td>\n",
              "      <td>0.279+/-0.008</td>\n",
              "      <td>0.468+/-0.009</td>\n",
              "      <td>0.280+/-0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.688+/-0.013</td>\n",
              "      <td>0.301+/-0.011</td>\n",
              "      <td>0.470+/-0.010</td>\n",
              "      <td>0.314+/-0.015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.665+/-0.011</td>\n",
              "      <td>0.273+/-0.010</td>\n",
              "      <td>0.447+/-0.069</td>\n",
              "      <td>0.277+/-0.014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.659+/-0.013</td>\n",
              "      <td>0.263+/-0.006</td>\n",
              "      <td>0.468+/-0.010</td>\n",
              "      <td>0.262+/-0.006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.598+/-0.003</td>\n",
              "      <td>0.199+/-0.001</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>250</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.683+/-0.005</td>\n",
              "      <td>0.288+/-0.009</td>\n",
              "      <td>0.472+/-0.005</td>\n",
              "      <td>0.293+/-0.016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>250</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.598+/-0.003</td>\n",
              "      <td>0.199+/-0.001</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>250</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.686+/-0.015</td>\n",
              "      <td>0.294+/-0.012</td>\n",
              "      <td>0.474+/-0.008</td>\n",
              "      <td>0.302+/-0.018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>250</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>250</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.670+/-0.010</td>\n",
              "      <td>0.277+/-0.008</td>\n",
              "      <td>0.445+/-0.071</td>\n",
              "      <td>0.282+/-0.014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>250</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>250</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.673+/-0.010</td>\n",
              "      <td>0.277+/-0.004</td>\n",
              "      <td>0.470+/-0.005</td>\n",
              "      <td>0.278+/-0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>250</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.678+/-0.005</td>\n",
              "      <td>0.282+/-0.004</td>\n",
              "      <td>0.469+/-0.003</td>\n",
              "      <td>0.284+/-0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.598+/-0.003</td>\n",
              "      <td>0.199+/-0.001</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.683+/-0.013</td>\n",
              "      <td>0.289+/-0.005</td>\n",
              "      <td>0.472+/-0.015</td>\n",
              "      <td>0.294+/-0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.672+/-0.015</td>\n",
              "      <td>0.283+/-0.005</td>\n",
              "      <td>0.431+/-0.048</td>\n",
              "      <td>0.287+/-0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.673+/-0.013</td>\n",
              "      <td>0.277+/-0.008</td>\n",
              "      <td>0.468+/-0.007</td>\n",
              "      <td>0.278+/-0.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Dimensions Activation                    Losses Optimizer Accurasy(+/-sd)  \\\n",
              "0         100    softmax  categorical_crossentropy      adam   0.676+/-0.017   \n",
              "1         100    softmax  categorical_crossentropy       sgd   0.600+/-0.000   \n",
              "2         100    softmax       binary_crossentropy      adam   0.688+/-0.013   \n",
              "3         100    softmax       binary_crossentropy       sgd   0.600+/-0.000   \n",
              "4         100    sigmoid  categorical_crossentropy      adam   0.665+/-0.011   \n",
              "5         100    sigmoid  categorical_crossentropy       sgd   0.600+/-0.000   \n",
              "6         100    sigmoid       binary_crossentropy      adam   0.659+/-0.013   \n",
              "7         100    sigmoid       binary_crossentropy       sgd   0.598+/-0.003   \n",
              "8         250    softmax  categorical_crossentropy      adam   0.683+/-0.005   \n",
              "9         250    softmax  categorical_crossentropy       sgd   0.598+/-0.003   \n",
              "10        250    softmax       binary_crossentropy      adam   0.686+/-0.015   \n",
              "11        250    softmax       binary_crossentropy       sgd   0.600+/-0.000   \n",
              "12        250    sigmoid  categorical_crossentropy      adam   0.670+/-0.010   \n",
              "13        250    sigmoid  categorical_crossentropy       sgd   0.600+/-0.000   \n",
              "14        250    sigmoid       binary_crossentropy      adam   0.673+/-0.010   \n",
              "15        250    sigmoid       binary_crossentropy       sgd   0.600+/-0.000   \n",
              "16        500    softmax  categorical_crossentropy      adam   0.678+/-0.005   \n",
              "17        500    softmax  categorical_crossentropy       sgd   0.598+/-0.003   \n",
              "18        500    softmax       binary_crossentropy      adam   0.683+/-0.013   \n",
              "19        500    softmax       binary_crossentropy       sgd   0.600+/-0.000   \n",
              "20        500    sigmoid  categorical_crossentropy      adam   0.672+/-0.015   \n",
              "21        500    sigmoid  categorical_crossentropy       sgd   0.600+/-0.000   \n",
              "22        500    sigmoid       binary_crossentropy      adam   0.673+/-0.013   \n",
              "23        500    sigmoid       binary_crossentropy       sgd   0.600+/-0.000   \n",
              "\n",
              "    Recall(+/-sd) Precision(+/-sd) F1_score(+/-sd)  \n",
              "0   0.279+/-0.008    0.468+/-0.009   0.280+/-0.008  \n",
              "1   0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "2   0.301+/-0.011    0.470+/-0.010   0.314+/-0.015  \n",
              "3   0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "4   0.273+/-0.010    0.447+/-0.069   0.277+/-0.014  \n",
              "5   0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "6   0.263+/-0.006    0.468+/-0.010   0.262+/-0.006  \n",
              "7   0.199+/-0.001    0.120+/-0.000   0.150+/-0.000  \n",
              "8   0.288+/-0.009    0.472+/-0.005   0.293+/-0.016  \n",
              "9   0.199+/-0.001    0.120+/-0.000   0.150+/-0.000  \n",
              "10  0.294+/-0.012    0.474+/-0.008   0.302+/-0.018  \n",
              "11  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "12  0.277+/-0.008    0.445+/-0.071   0.282+/-0.014  \n",
              "13  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "14  0.277+/-0.004    0.470+/-0.005   0.278+/-0.005  \n",
              "15  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "16  0.282+/-0.004    0.469+/-0.003   0.284+/-0.003  \n",
              "17  0.199+/-0.001    0.120+/-0.000   0.150+/-0.000  \n",
              "18  0.289+/-0.005    0.472+/-0.015   0.294+/-0.008  \n",
              "19  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "20  0.283+/-0.005    0.431+/-0.048   0.287+/-0.003  \n",
              "21  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "22  0.277+/-0.008    0.468+/-0.007   0.278+/-0.009  \n",
              "23  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "j__OM8dhqWGT",
        "colab_type": "code",
        "outputId": "29a51974-1c6d-46ad-86e4-a6bd465e7838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "cell_type": "code",
      "source": [
        "total.sort_values('Accurasy(+/-sd)',ascending=False)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dimensions</th>\n",
              "      <th>Activation</th>\n",
              "      <th>Losses</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>Accurasy(+/-sd)</th>\n",
              "      <th>Recall(+/-sd)</th>\n",
              "      <th>Precision(+/-sd)</th>\n",
              "      <th>F1_score(+/-sd)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.688+/-0.013</td>\n",
              "      <td>0.301+/-0.011</td>\n",
              "      <td>0.470+/-0.010</td>\n",
              "      <td>0.314+/-0.015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>250</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.686+/-0.015</td>\n",
              "      <td>0.294+/-0.012</td>\n",
              "      <td>0.474+/-0.008</td>\n",
              "      <td>0.302+/-0.018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.683+/-0.013</td>\n",
              "      <td>0.289+/-0.005</td>\n",
              "      <td>0.472+/-0.015</td>\n",
              "      <td>0.294+/-0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>250</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.683+/-0.005</td>\n",
              "      <td>0.288+/-0.009</td>\n",
              "      <td>0.472+/-0.005</td>\n",
              "      <td>0.293+/-0.016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.678+/-0.005</td>\n",
              "      <td>0.282+/-0.004</td>\n",
              "      <td>0.469+/-0.003</td>\n",
              "      <td>0.284+/-0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.676+/-0.017</td>\n",
              "      <td>0.279+/-0.008</td>\n",
              "      <td>0.468+/-0.009</td>\n",
              "      <td>0.280+/-0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.673+/-0.013</td>\n",
              "      <td>0.277+/-0.008</td>\n",
              "      <td>0.468+/-0.007</td>\n",
              "      <td>0.278+/-0.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>250</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.673+/-0.010</td>\n",
              "      <td>0.277+/-0.004</td>\n",
              "      <td>0.470+/-0.005</td>\n",
              "      <td>0.278+/-0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.672+/-0.015</td>\n",
              "      <td>0.283+/-0.005</td>\n",
              "      <td>0.431+/-0.048</td>\n",
              "      <td>0.287+/-0.003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>250</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.670+/-0.010</td>\n",
              "      <td>0.277+/-0.008</td>\n",
              "      <td>0.445+/-0.071</td>\n",
              "      <td>0.282+/-0.014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.665+/-0.011</td>\n",
              "      <td>0.273+/-0.010</td>\n",
              "      <td>0.447+/-0.069</td>\n",
              "      <td>0.277+/-0.014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.659+/-0.013</td>\n",
              "      <td>0.263+/-0.006</td>\n",
              "      <td>0.468+/-0.010</td>\n",
              "      <td>0.262+/-0.006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>250</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>250</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>250</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.600+/-0.000</td>\n",
              "      <td>0.200+/-0.000</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>250</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.598+/-0.003</td>\n",
              "      <td>0.199+/-0.001</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.598+/-0.003</td>\n",
              "      <td>0.199+/-0.001</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.598+/-0.003</td>\n",
              "      <td>0.199+/-0.001</td>\n",
              "      <td>0.120+/-0.000</td>\n",
              "      <td>0.150+/-0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Dimensions Activation                    Losses Optimizer Accurasy(+/-sd)  \\\n",
              "2         100    softmax       binary_crossentropy      adam   0.688+/-0.013   \n",
              "10        250    softmax       binary_crossentropy      adam   0.686+/-0.015   \n",
              "18        500    softmax       binary_crossentropy      adam   0.683+/-0.013   \n",
              "8         250    softmax  categorical_crossentropy      adam   0.683+/-0.005   \n",
              "16        500    softmax  categorical_crossentropy      adam   0.678+/-0.005   \n",
              "0         100    softmax  categorical_crossentropy      adam   0.676+/-0.017   \n",
              "22        500    sigmoid       binary_crossentropy      adam   0.673+/-0.013   \n",
              "14        250    sigmoid       binary_crossentropy      adam   0.673+/-0.010   \n",
              "20        500    sigmoid  categorical_crossentropy      adam   0.672+/-0.015   \n",
              "12        250    sigmoid  categorical_crossentropy      adam   0.670+/-0.010   \n",
              "4         100    sigmoid  categorical_crossentropy      adam   0.665+/-0.011   \n",
              "6         100    sigmoid       binary_crossentropy      adam   0.659+/-0.013   \n",
              "11        250    softmax       binary_crossentropy       sgd   0.600+/-0.000   \n",
              "1         100    softmax  categorical_crossentropy       sgd   0.600+/-0.000   \n",
              "13        250    sigmoid  categorical_crossentropy       sgd   0.600+/-0.000   \n",
              "15        250    sigmoid       binary_crossentropy       sgd   0.600+/-0.000   \n",
              "5         100    sigmoid  categorical_crossentropy       sgd   0.600+/-0.000   \n",
              "19        500    softmax       binary_crossentropy       sgd   0.600+/-0.000   \n",
              "21        500    sigmoid  categorical_crossentropy       sgd   0.600+/-0.000   \n",
              "3         100    softmax       binary_crossentropy       sgd   0.600+/-0.000   \n",
              "23        500    sigmoid       binary_crossentropy       sgd   0.600+/-0.000   \n",
              "9         250    softmax  categorical_crossentropy       sgd   0.598+/-0.003   \n",
              "7         100    sigmoid       binary_crossentropy       sgd   0.598+/-0.003   \n",
              "17        500    softmax  categorical_crossentropy       sgd   0.598+/-0.003   \n",
              "\n",
              "    Recall(+/-sd) Precision(+/-sd) F1_score(+/-sd)  \n",
              "2   0.301+/-0.011    0.470+/-0.010   0.314+/-0.015  \n",
              "10  0.294+/-0.012    0.474+/-0.008   0.302+/-0.018  \n",
              "18  0.289+/-0.005    0.472+/-0.015   0.294+/-0.008  \n",
              "8   0.288+/-0.009    0.472+/-0.005   0.293+/-0.016  \n",
              "16  0.282+/-0.004    0.469+/-0.003   0.284+/-0.003  \n",
              "0   0.279+/-0.008    0.468+/-0.009   0.280+/-0.008  \n",
              "22  0.277+/-0.008    0.468+/-0.007   0.278+/-0.009  \n",
              "14  0.277+/-0.004    0.470+/-0.005   0.278+/-0.005  \n",
              "20  0.283+/-0.005    0.431+/-0.048   0.287+/-0.003  \n",
              "12  0.277+/-0.008    0.445+/-0.071   0.282+/-0.014  \n",
              "4   0.273+/-0.010    0.447+/-0.069   0.277+/-0.014  \n",
              "6   0.263+/-0.006    0.468+/-0.010   0.262+/-0.006  \n",
              "11  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "1   0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "13  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "15  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "5   0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "19  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "21  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "3   0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "23  0.200+/-0.000    0.120+/-0.000   0.150+/-0.000  \n",
              "9   0.199+/-0.001    0.120+/-0.000   0.150+/-0.000  \n",
              "7   0.199+/-0.001    0.120+/-0.000   0.150+/-0.000  \n",
              "17  0.199+/-0.001    0.120+/-0.000   0.150+/-0.000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "Q35XDBrBqVtd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NeM4vMV3LSwa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gtbiojtQLSg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a1_Q-WPaLSQw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m-Y07GaMLR9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FnvSIPZLRuq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oWzbaUB_Gmcu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vector Semantics - Learn word embeddings\n",
        "** Get word embeddings with Word2Vec.**\n",
        "<br>\n",
        "Begin by reading the gensim documentation for [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html), to figure out how to use the Word2Vec class. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_jAozyC_9jTQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"20px\" src=\"http://mlbernauer.github.io/assets/python.png\" align=\"left\" hspace=\"5px\" vspace=\"3px\">\n",
        "<h3> Exercise </h3>\n",
        "\n",
        "1. Read the Word2Vec documentation, and\n",
        "2. train a Word2Vec model that learns embeddings in $\\mathbb R^{100}$  from the TED dataset using SkipGram model. Other options should be default except min_count=10 so that infrequent words are ignored.  \n",
        "3. train a Word2Vec model that learns embeddings in $\\mathbb R^{10}$  from the TED dataset using SkipGram model. \n",
        "4. Get the most similar word to \"computer\" for both models. Do you notice any difference? What exactly?\n"
      ]
    },
    {
      "metadata": {
        "id": "O7LIBvgz5XJs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Nax-M_PtgKt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XR1wU-yhtgAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuUzyLgmtf2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "euafd6sAtfrT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5DLx-lctfgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tN3D5-GMtfU0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IAa25SeBtfCg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n29EY7iDIiUH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bujy3SkVGpil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GDK_P3_ZOxXS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Network Classification models\n",
        "Neural Network Models address the n-gram data sparsity issue through parameterization of words as vectors (word embeddings) and using them as inputs to a neural network.\n"
      ]
    },
    {
      "metadata": {
        "id": "z16LUFSVUV1q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Movie Review Polarity Dataset\n",
        "The Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in the early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available as part of their research.\n",
        "The dataset is comprised of **1,000 positive** and **1,000 negative movie reviews** drawn from an archive of the rec.arts.movies.reviews newsgroup hosted at imdb.com. The authors refer to this dataset as the “polarity dataset.”\n",
        "\n",
        "**You can download the dataset from here**:\n",
        "[Movie Review Polarity Dataset (review_polarity.tar.gz, 3MB)]()\n",
        "\n",
        "After unzipping the file, you will have a directory called “txt_sentoken” with two sub-directories containing the text “neg” and “pos” for negative and positive reviews. Reviews are stored one per file with a naming convention cv000 to cv999 for each neg and pos.\n",
        "\n",
        "**The data has been cleaned up:**\n",
        "* The dataset is comprised of only English reviews.\n",
        "* All text has been converted to lowercase.\n",
        "* There is white space around punctuation like periods, commas, and brackets.\n",
        "* Text has been split into one sentence per line."
      ]
    },
    {
      "metadata": {
        "id": "e384ZS2Z2sqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2 Data Preparation\n",
        " Prepare movie review text data for classification with neural network methods\n",
        " \n",
        " \n",
        "\n",
        "*   Split into Train / Validation sets\n",
        "*   Loading & Cleaning Reviews\n",
        "*   Define a vocabulary of preferred words\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "v-322eST-K6q",
        "colab_type": "code",
        "outputId": "0ac740b1-74ab-4749-b549-ed54cbce22f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w6moFoigSOOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data paths\n",
        "negativeReviewsDirectory = '/content/drive/My Drive/Colab Notebooks/Datasets/review_polarity/txt_sentoken/neg'\n",
        "positiveReviewsDirectory = '/content/drive/My Drive/Colab Notebooks/Datasets/review_polarity/txt_sentoken/pos'\n",
        "\n",
        "vocab_filename = '/content/drive/My Drive/Colab Notebooks/Datasets/vocab.txt'\n",
        "embedding_word2vec_filename = '/content/drive/My Drive/Colab Notebooks/Datasets/embedding_word2vec.txt'\n",
        "glove_embedding = '/content/drive/My Drive/glove.6B.100d.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gfZHIF4Y_Z3e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# filter out stop words\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\t# filter out short tokens\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\t# load doc\n",
        "\tdoc = load_doc(filename)\n",
        "\t# clean doc\n",
        "\ttokens = clean_doc(doc)\n",
        "\t# update counts\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# add doc to vocab\n",
        "\t\tadd_doc_to_vocab(path, vocab)\n",
        "\n",
        "\n",
        "# Add all docs to vocab\n",
        "# Define vocabulary\n",
        "vocab = Counter()\n",
        "# Get positive and negative documents from respective directories\n",
        "process_docs(negativeReviewsDirectory, vocab, True)\n",
        "process_docs(positiveReviewsDirectory, vocab, True)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(\"\\nVocabulary size:\", len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(\"\\nMost common words: \\n\", vocab.most_common(50))\n",
        "\n",
        "# Keep tokens with a min occurence\n",
        "min_occurence = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurence]\n",
        "print(\"\\nUpdated Vocabulary size:\", len(tokens))\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "\t# convert lines to a single blob of text\n",
        "\tdata = '\\n'.join(lines)\n",
        "\t# open file\n",
        "\tfile = open(filename, 'w')\n",
        "\t# write text\n",
        "\tfile.write(data)\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, vocab_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ELZHvfMSH9_Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoOPsOCk2snE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3 Train an Embedding Layer\n",
        "Word embeddings as part of fitting a neural network model.\n",
        "\n",
        "\n",
        "*   [Embedding layer](https://keras.io/layers/embeddings/) in Keras deep learning library\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VgzlwB3mM6LW",
        "colab_type": "code",
        "outputId": "f67d75cf-49a6-41c7-d0e4-147b87aaf4b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# filter out tokens not in vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\tdocuments = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\t# clean doc\n",
        "\t\ttokens = clean_doc(doc, vocab)\n",
        "\t\t# add to list\n",
        "\t\tdocuments.append(tokens)\n",
        "\treturn documents\n",
        "\n",
        "# load all training reviews\n",
        "positive_docs = process_docs(positiveReviewsDirectory, vocab, True)\n",
        "negative_docs = process_docs(negativeReviewsDirectory, vocab, True)\n",
        "train_docs = negative_docs + positive_docs\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define training labels\n",
        "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
        "\n",
        "# load all test reviews\n",
        "positive_docs = process_docs(positiveReviewsDirectory, vocab, False)\n",
        "negative_docs = process_docs(negativeReviewsDirectory, vocab, False)\n",
        "test_docs = negative_docs + positive_docs\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "# pad sequences\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define test labels\n",
        "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Define Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0a1ef8b744a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# load all training reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mpositive_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositiveReviewsDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mnegative_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegativeReviewsDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mtrain_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_docs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpositive_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0a1ef8b744a8>\u001b[0m in \u001b[0;36mprocess_docs\u001b[0;34m(directory, vocab, is_trian)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# walk through all files in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# skip any reviews in the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_trian\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cv9'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/txt_sentoken/pos'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6ys-qW_-M6HQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z0Bi_7d32shn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.4 Train a Word2Vec Embedding"
      ]
    },
    {
      "metadata": {
        "id": "7AqXlgZdPPNV",
        "colab_type": "code",
        "outputId": "8e5c0145-5e3e-4233-a8ae-19e611d4b5a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def doc_to_clean_lines(doc, vocab):\n",
        "\tclean_lines = list()\n",
        "\tlines = doc.splitlines()\n",
        "\tfor line in lines:\n",
        "\t\t# split into tokens by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# remove punctuation from each token\n",
        "\t\ttable = str.maketrans('', '', punctuation)\n",
        "\t\ttokens = [w.translate(table) for w in tokens]\n",
        "\t\t# filter out tokens not in vocab\n",
        "\t\ttokens = [w for w in tokens if w in vocab]\n",
        "\t\tclean_lines.append(tokens)\n",
        "\treturn clean_lines\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_trian):\n",
        "\tlines = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_trian and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load and clean the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\tdoc_lines = doc_to_clean_lines(doc, vocab)\n",
        "\t\t# add lines to list\n",
        "\t\tlines += doc_lines\n",
        "\treturn lines\n",
        "\n",
        "\n",
        "# Get all training reviews\n",
        "positive_lines = process_docs(positiveReviewsDirectory, vocab, True)\n",
        "negative_lines = process_docs(negativeReviewsDirectory, vocab, True)\n",
        "sentences = positive_lines + negative_lines\n",
        "print('Total training sentences: %d' % len(sentences))\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences, size=100, window=5, workers=-1, min_count=10)\n",
        "# summarize vocabulary size in model\n",
        "words = list(model.wv.vocab)\n",
        "print('Vocabulary size: %d' % len(words))\n",
        "\n",
        "# save model in ASCII (word2vec) format\n",
        "model.wv.save_word2vec_format(embedding_word2vec_filename, binary=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training sentences: 58109\n",
            "Vocabulary size: 8465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6t4ece883QhU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.5 Use pre-trained Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "MLBlz1zRUCtw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.5.1 Use pre-trained in-domain Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "fIjE196JR-pH",
        "colab_type": "code",
        "outputId": "e10f1fb4-a9c7-4ec3-a72d-7e9c90bdbd68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# filter out tokens not in vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\tdocuments = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\t# clean doc\n",
        "\t\ttokens = clean_doc(doc, vocab)\n",
        "\t\t# add to list\n",
        "\t\tdocuments.append(tokens)\n",
        "\treturn documents\n",
        "\n",
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "\t# load embedding into memory, skip first line\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()[1:]\n",
        "\tfile.close()\n",
        "\t# create a map of words to vectors\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding\n",
        "\n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = zeros((vocab_size, 100))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tweight_matrix[i] = embedding.get(word)\n",
        "\treturn weight_matrix\n",
        "\n",
        "\n",
        "# load all training reviews\n",
        "positive_docs = process_docs(positiveReviewsDirectory, vocab, True)\n",
        "negative_docs = process_docs(negativeReviewsDirectory, vocab, True)\n",
        "train_docs = negative_docs + positive_docs\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define training labels\n",
        "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
        "\n",
        "# load all test reviews\n",
        "positive_docs = process_docs(positiveReviewsDirectory, vocab, False)\n",
        "negative_docs = process_docs(negativeReviewsDirectory, vocab, False)\n",
        "test_docs = negative_docs + positive_docs\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "# pad sequences\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define test labels\n",
        "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# load embedding from file\n",
        "raw_embedding = load_embedding(embedding_word2vec_filename)\n",
        "# get vectors in the right order\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "# create the embedding layer\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=True)\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(\"\\nModel Summary:\\n\", model.summary())\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('\\nTest Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4a72aad686a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# load all training reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mpositive_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositiveReviewsDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mnegative_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegativeReviewsDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mtrain_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_docs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpositive_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "xVABRNWvTltH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XSLV7SwBVuYk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.5.2 Use pre-trained word Embeddings on external datasets\n",
        "Google and Stanford provide pre-trained word vectors that you can download, trained with the efficient word2vec and GloVe methods respectively.\n",
        "We will use the [pre-trained GloVe vectors](https://nlp.stanford.edu/projects/glove/) from the Stanford webpage, which are trained on Wikipedia data. "
      ]
    },
    {
      "metadata": {
        "id": "4q2NtQ3ibE0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ni-wQbcfb_b5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img height=\"20px\" src=\"http://mlbernauer.github.io/assets/python.png\" align=\"left\" hspace=\"5px\" vspace=\"3px\">\n",
        "<h3> Exercise </h3>\n",
        "\n",
        "Train a neural network classifier on Movie Review Polarity Dataset using the pre-trained GloVe word embeddings. You can use the code from 'building a text classifier using in-domain Embeddings' above as basis. What modifications are essential when using embeddings from an external dataset? Think and modify the code accordingly."
      ]
    },
    {
      "metadata": {
        "id": "ekC9i1pGTlqC",
        "colab_type": "code",
        "outputId": "820084be-df2d-49aa-c4ea-f685517f1528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# filter out tokens not in vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "\tdocuments = list()\n",
        "\t# walk through all files in the folder\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# skip any reviews in the test set\n",
        "\t\tif is_train and filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\tif not is_train and not filename.startswith('cv9'):\n",
        "\t\t\tcontinue\n",
        "\t\t# create the full path of the file to open\n",
        "\t\tpath = directory + '/' + filename\n",
        "\t\t# load the doc\n",
        "\t\tdoc = load_doc(path)\n",
        "\t\t# clean doc\n",
        "\t\ttokens = clean_doc(doc, vocab)\n",
        "\t\t# add to list\n",
        "\t\tdocuments.append(tokens)\n",
        "\treturn documents\n",
        "\n",
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "\t# load embedding into memory, skip first line\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()[1:]\n",
        "\tfile.close()\n",
        "\t# create a map of words to vectors\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding\n",
        "\n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = zeros((vocab_size, 100))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tweight_matrix[i] = embedding.get(word)\n",
        "\treturn weight_matrix\n",
        "\n",
        "\n",
        "# load all training reviews\n",
        "positive_docs = process_docs(positiveReviewsDirectory, vocab, True)\n",
        "negative_docs = process_docs(negativeReviewsDirectory, vocab, True)\n",
        "train_docs = negative_docs + positive_docs\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define training labels\n",
        "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
        "\n",
        "# load all test reviews\n",
        "positive_docs = process_docs(positiveReviewsDirectory, vocab, False)\n",
        "negative_docs = process_docs(negativeReviewsDirectory, vocab, False)\n",
        "test_docs = negative_docs + positive_docs\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "# pad sequences\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define test labels\n",
        "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# load embedding from file\n",
        "raw_embedding = load_embedding(embedding_word2vec_filename)\n",
        "# get vectors in the right order\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "# create the embedding layer\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=True)\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(\"\\nModel Summary:\\n\", model.summary())\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('\\nTest Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-4a72aad686a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# load all training reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mpositive_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositiveReviewsDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mnegative_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegativeReviewsDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mtrain_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_docs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpositive_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'positiveReviewsDirectory' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "xBscvZOQTlnH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XHhVoUDPZpVP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zc4YyLihb8fp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thank You! "
      ]
    },
    {
      "metadata": {
        "id": "Hv3hwEULdxfI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}